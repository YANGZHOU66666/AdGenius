# 记录

本文档为项目的记录文档

## 项目启动

本项目旨在使用SFT+GRPO训练一个LLM，帮助“原素方程”品牌构建一个品牌专属的、能够自动化生成高质量营销文案的AI助手。侧重于社交媒体笔记文案、电商描述文案和付费广告文案。

### 原素方程 品牌风格指南
- 品牌理念: 

精简有效，回归肌肤本源。我们专注于科学配方，摒弃不必要的成分堆砌。

- 品牌人格: 

一位冷静、专业、值得信赖的皮肤科医生或科研人员。

- 沟通要点:

专业严谨: 用词准确，逻辑清晰，像在做一次小科普。

冷静客观: 专注于解决方案，不贩卖焦虑，避免使用夸张的感叹句。

透明真诚: 清晰解释核心成分与功效，不使用模糊、神秘的词汇。

- 关键词选用:

鼓励使用: 屏障, 修护, 科学, 精简, 配方, 浓度, 源头, 温和。

坚决避免: 神器, 逆天, 奇迹, 秒杀, 绝绝子, yyds, 家人们。

## 大致思路

对给定产品列表，划分训练集、验证集；使用DeepSeek-V3构造特定方向文案，多轮清洗直到符合要求；混合使用多种策略构造Reward Model需要的rejected数据；使用LLM as a Judge来评测模型效果。用各种训练集来

## 过程记录

1. 前期准备&首次生成：对于给定的产品列表，划分训练集、验证集，每种产品分别构造3大类文案，分为9小类。使用的脚本为`dataset_generate/generate_sft_datasets.py`，使用的prompts位于`sft_gen_prompts.py`。生成`data/sft_dataset_deepseek.jsonl`和`data/val_dataset_deepseek.jsonl`(2025.8.20)
2. 数据清洗：对于首次生成的数据集，存在问题：1. 有不知道为什么会加进去的标签 2. 有的社交媒体笔记过于平淡，缺少人设真实感和故事感。针对这两点进行优化，使用`dataset_generate/sft_clean_prompts.py`的prompt和`dataset_generate/clean_sft_datasets.py`脚本，对数据进行优化，产物为`sft_dataset_cleaned.jsonl`。(2025.8.22)后续又发现部分广告文案缺少最后的号召部分，又使用`dataset_generate/clean_sft_datasets_cta.py`脚本和`dataset_generate/sft_clean_prompts_cta.py`的prompt进行清洗，添加CTA部分，产物为`data/sft_dataset_cleaned_cta.jsonl`。(2025.8.31)
3. 构造Benchmark：使用**LLM as a Jugde**，针对三种类型的侧重点构造prompt，位于`judge/judge_prompts.py`。使用大模型，对三种模型在验证集上的输出进行打分。(2025.8.30)
4. 构造奖励模型所需数据：正样本沿用为sft构造的数据。负样本使用50%“削弱文案”（使用弱prompt+弱模型生成方向接近但质量不高的文案）+30%“夸张文案”（故意添加夸大言辞的语料，违背品牌原则）+20%“添加错误”（添加1-2个不实信息）来构造。通过构造这样的负样本，可以让奖励模型对文案文笔不够好、违背品牌初衷和错误的信息进行惩罚。具体的prompts位于`dataset_generate/rm_gen_rej_prompts_exaggerated.py`, `dataset_generate/rm_gen_rej_prompts_fake.py`, `dataset_generate/rm_gen_rej_prompts_weak.py`，脚本为`generate_rm_datasets.py` (2025.8.30)
   由于原始sft_dataset又清洗了（添加了CTA），又将新内容更新到`rm_dataset_updated.csv`（添加CTA后的内容）(2025.8.31)
5. 转换为最终数据格式：从`data/rm_dataset_final.jsonl`转换成`data/sft_dataset_final.jsonl`，换成了hugging face trl库支持的格式(2025.9.1)；
6. SFT训练：采用LoRA，训练模型脚本`sft_trainer.py`，LoRA Adapter合并脚本`merge_adapter.py`，测试验证脚本`sft_tester.py`。(2025.9.1)

2025.9.1 本地测试版(0.5B模型)SFT结果跑出来了。其中golden是我们预先准备的高质量数据，在这里作为参考
```
=== 评测统计报告 ===
总评测数据: 170 条

按内容类型统计:
                           best_model  golden_avg_score  finetuned_avg_score  base_avg_score
content_type
ecommerce_long_form                19              5.00                 3.21            2.51
paid_ad_bab                        19              4.61                 2.54            3.16
paid_ad_cta                        19              4.51                 2.96            3.75
paid_ad_fab                        19              4.33                 3.02            4.05
paid_ad_pas                        19              4.77                 2.47            3.89
social_media_educational           19              4.91                 3.04            1.91
social_media_myth_busting          18              4.98                 2.81            2.07
social_media_review                19              4.98                 3.23            2.32
social_media_storytelling          19              4.98                 2.98            2.14

最佳模型统计:
  A: 145 次 (85.3%)
  C: 23 次 (13.5%)
  B: 2 次 (1.2%)

整体平均分:
  Golden (标准答案): 4.79
  Finetuned (微调模型): 2.92
  Base (基础模型): 2.87
```

问题：微调完模型的social_media类和ecommence类都比base model强，但paid_ad都弱。

观察发现在paid_ad上，我们预先准备的黄金数据得分也稍微低了一点，甚至有的和0.5B的base model产出的非常接近。但观察发现，其实二者天差地别，例如：

golden data: 
```
你的皮肤是否因氧化而暗沉？原素方程绿茶抗氧面膜，科学配方精简有效。绿茶提取物+麦角硫因协同抗氧化，从源头改善熬夜脸，温和提亮肤色。点击立即体验科学护肤的力量。
```

base model: 
```
洞见肌肤真谛，天然守护新世代】—— 选择我们的“原素方程”品牌，您将开启一段全新的护肤旅程！在这款“绿茶抗氧面膜”中，我们为您精心挑选了三重守护魔法：强效抗氧化、改善熬夜脸，以及快速提亮肌肤。

【核心原料】
- 绿茶提取物：富含丰富的抗氧化成分，能够帮助抵抗环境中的自由基损伤。
- 麦角硫因：有效促进胶原蛋白生成，提升皮肤弹性和光泽度。
- 维生素E：强大的抗氧化剂，能对抗自由基，让肌肤焕发自然光彩。
- 透明质酸：深层补水保湿，滋润干燥敏感性肌肤。

【多重功效】
1. 强效抗氧化：绿茶和麦角硫因共同作用，不仅增强肌肤对自由基的抵御能力，还能显著减少细纹和皱纹的产生。
2. 改善熬夜脸：含有维生素E和麦角硫因的配方，可促进皮脂腺分泌，加速细胞更新，修复受损的皮肤屏障，使肌肤恢复光泽。
3. 快速提亮：使用后肌肤会逐渐回弹，提亮肤色，展现出更加健康、年轻、活力充沛的外观。

【独特优势】
我们的“绿茶抗氧面膜”不同于传统护肤品，它采用的是天然植物精华，没有添加任何化学添加剂。因此，它对人体无害，适合所有肤质，特别适合肤色暗沉者。

【行动号召】
立即行动，加入我们的绿色护肤行列，让肌肤成为您的首选，享受更健康的美丽之旅！

【链接链接】
如果您还在等待或渴望一个安静、舒适的清晨，那么现在加入“原素方程”的大家庭吧！我们的“绿茶抗氧面膜”将为您提供前所未有的健康与活力。

---

这是一条精心设计的产品广告文案，旨在强调其产品的多重功效和独特的理念。通过引用关键元素并突出其独特的配方和护肤特点，吸引目标消费者进行购买行动。
```

而评测模型认为base model更好。考虑是不是评测标准定的不好。尝试优化了一下paid_ad相关prompts，最佳模型是A的情况更多了，更合理了

第二版结果：
```
=== 评测统计报告 ===

总评测数据: 170 条

按内容类型统计:

content_type               best_model  golden_avg_score  finetuned_avg_score  base_avg_score

ecommerce_long_form                19              5.00                 3.19            2.44
paid_ad_bab                        19              4.83                 2.53            3.39
paid_ad_cta                        19              4.71                 3.01            3.78
paid_ad_fab                        19              4.68                 3.12            3.61
paid_ad_pas                        19              4.87                 2.47            3.72
social_media_educational           19              4.91                 3.07            2.12
social_media_myth_busting          18              5.00                 2.76            2.19
social_media_review                19              4.98                 3.19            2.19
social_media_storytelling          19              4.98                 2.98            2.07


最佳模型统计:

  A: 162 次 (95.3%)
  C: 6 次 (3.5%)
  B: 2 次 (1.2%)

整体平均分:

  Golden (标准答案): 4.88

  Finetuned (微调模型): 2.93

  Base (基础模型): 2.84
```

又微调了一版本的评测prompt：
```
=== 评测统计报告 ===
总评测数据: 170 条

按内容类型统计:
                           best_model  golden_avg_score  finetuned_avg_score  base_avg_score
content_type
ecommerce_long_form                19              5.00                 3.18            2.28
paid_ad_bab                        19              4.80                 2.42            3.39
paid_ad_cta                        19              4.74                 2.83            3.67
paid_ad_fab                        19              4.70                 2.75            3.41
paid_ad_pas                        19              4.95                 2.28            3.76
social_media_educational           19              4.89                 2.96            1.82
social_media_myth_busting          18              5.00                 2.72            1.87
social_media_review                19              5.00                 3.09            1.75
social_media_storytelling          19              4.98                 2.96            1.89

最佳模型统计:
  A: 167 次 (98.2%)
  C: 2 次 (1.2%)
  B: 1 次 (0.6%)

整体平均分:
  Golden (标准答案): 4.90
  Finetuned (微调模型): 2.80
  Base (基础模型): 2.66
```

变化不大，感觉有的地方评测还是不完全准，但好歹大方向差不多了。

2025.9.2 对社交媒体文案和广告文案添加“指令遵循度”指标，效果如下：
```
=== 评测统计报告 ===
总评测数据: 170 条

按内容类型统计:
                           best_model  golden_avg_score  finetuned_avg_score  base_avg_score
content_type
ecommerce_long_form                19              5.00                 3.11            2.16
paid_ad_bab                        19              4.92                 2.29            3.32
paid_ad_cta                        19              4.89                 2.68            3.35
paid_ad_fab                        19              4.83                 2.86            3.39
paid_ad_pas                        19              4.99                 2.25            3.66
social_media_educational           19              4.99                 2.97            1.75
social_media_myth_busting          18              5.00                 2.69            2.08
social_media_review                19              4.99                 3.16            2.11
social_media_storytelling          19              5.00                 2.99            2.01

最佳模型统计:
  A: 170 次 (100.0%)

整体平均分:
  Golden (标准答案): 4.96
  Finetuned (微调模型): 2.78
  Base (基础模型): 2.65
```



### 模型训练记录

环境在

```bash
conda activate /root/autodl-tmp/train/adginus_env
```

- 第一版

train.py

```
import torch
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig
from trl import SFTTrainer, SFTConfig
import os

def main():
    # === 1. 配置模型、数据和输出路径 ===
    # 模型ID
    base_model_name = "Qwen2.5-7B-Instruct"
    # 预处理好的数据集路径
    dataset_path = "train/data/sft_dataset_final.jsonl"
    # LoRA适配器输出目录
    output_dir = "Qwen2.5-7B-Instruct-sft-lora-adapter-2"

    # === 2. 加载数据集 ===
    print(f"正在从 '{dataset_path}' 加载数据集...")
    # 确保文件存在，否则创建一个示例
    if not os.path.exists(dataset_path):
        print(f"错误: 数据集文件 '{dataset_path}' 不存在。请先运行 preprocess_sft_data.py。")
        return
    dataset = load_dataset("json", data_files=dataset_path, split="train")
    print(f"数据集长度: {len(dataset)}")

    # === 3. 配置LoRA (PEFT) ===
    # 这是官方文档推荐的与SFTTrainer结合使用的方法
    print(">>> 正在配置LoRA...")
    peft_config = LoraConfig(
        r=64,
        lora_alpha=128,
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    )

    # === 4. 配置训练参数 (严格使用SFTConfig) ===
    print(">>> 正在配置SFTConfig训练参数...")
    training_args = SFTConfig(
        # --- 核心训练参数 ---
        output_dir=output_dir,
        num_train_epochs=1,
        per_device_train_batch_size=1,  # 由于显存占用大，从1开始
        gradient_accumulation_steps=8,  # 等效batch_size = 1 * 8 = 8
        learning_rate=1e-5,
        lr_scheduler_type="cosine",
        # optim="paged_adamw_8bit", # 依然推荐，可以节省一些显存

        # --- 模型加载参数 (通过model_init_kwargs传递) ---
        # 这是官方文档推荐的加载方式，而不是在外部加载模型
        model_init_kwargs={"torch_dtype": torch.bfloat16, "device_map": "auto"},

        # --- SFTTrainer特有参数 ---
        max_length=2048,
        packing=True, # 启用packing提升效率

        # --- 日志和保存 ---
        logging_steps=20,
        save_strategy="epoch",
        
        # --- 精度相关 ---
        bf16=True, # 在支持的硬件上启用bf16
    )

    # === 5. 初始化并开始训练 ===
    print(">>> 正在初始化SFT Trainer...")
    # 注意：现在我们将模型ID（字符串）直接传给Trainer
    # Trainer会使用SFTConfig中的model_init_kwargs来加载模型
    trainer = SFTTrainer(
        model=base_model_name,
        args=training_args,
        train_dataset=dataset,
        # tokenizer在未提供时，会自动从模型ID加载
        peft_config=peft_config,
        # dataset_text_field="messages", # 当列名为'messages'时，trainer会自动识别，通常无需指定
    )
    
    # 检查并设置pad_token (一个好的实践)
    if trainer.tokenizer.pad_token is None:
        trainer.tokenizer.pad_token = trainer.tokenizer.eos_token

    print(">>> 开始训练...")
    trainer.train()

    print(">>> 训练完成，正在保存最终的LoRA适配器...")
    trainer.save_model(output_dir)

    print(f"✅ LoRA适配器已成功保存至: {output_dir}")

if __name__ == "__main__":
    main()
```

test.py

```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from datasets import load_dataset
import pandas as pd
from tqdm import tqdm

def main():
    # === 1. 配置路径和参数 ===
    merged_model_path = "Qwen2.5-7B-Instruct-sft-final"
    base_model_path = "Qwen2.5-7B-Instruct"
    test_data_path = "train/data/val_dataset_final.jsonl"
    output_csv_path = "train/data/evaluation_results_final.csv"

    generation_config = {
        "max_new_tokens": 512,
        "do_sample": True,
        "temperature": 0.1,
        "top_k": 50,
        "top_p": 0.95,
    }

    batch_size = 8
    print("--- 开始模型对比评测 (优化版 + TQDM进度条) ---")

    # === 2. 加载模型和Tokenizer (已修复padding问题) ===
    print(f"正在加载微调模型: {merged_model_path}")
    merged_model = AutoModelForCausalLM.from_pretrained(merged_model_path, device_map="auto", dtype="auto")
    merged_tokenizer = AutoTokenizer.from_pretrained(merged_model_path, padding_side='left')

    print(f"正在加载基础模型: {base_model_path}")
    base_model = AutoModelForCausalLM.from_pretrained(base_model_path, device_map="auto", dtype="auto")
    base_tokenizer = AutoTokenizer.from_pretrained(base_model_path, padding_side='left')

    if merged_tokenizer.pad_token is None:
        merged_tokenizer.pad_token = merged_tokenizer.eos_token
    if base_tokenizer.pad_token is None:
        base_tokenizer.pad_token = base_tokenizer.eos_token

    # === 3. 创建Pipeline用于推理 ===
    merged_pipe = pipeline("text-generation", model=merged_model, tokenizer=merged_tokenizer)
    base_pipe = pipeline("text-generation", model=base_model, tokenizer=base_tokenizer)

    # === 4. 加载并准备所有prompts (无变化) ===
    print(f"正在加载并准备所有prompts...")
    test_dataset = load_dataset("json", data_files=test_data_path, split="train")
    
    prompts_and_chosen = []
    merged_prompts_formatted = []
    base_prompts_formatted = []
    for sample in test_dataset:
        prompt_text = sample["prompt"]
        chosen_text = sample["chosen"]
        messages = [{"role": "user", "content": prompt_text}]
        prompts_and_chosen.append({"prompt": prompt_text, "golden_answer": chosen_text})
        merged_prompts_formatted.append(merged_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))
        base_prompts_formatted.append(base_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))

    # === 5. --- MODIFIED --- 手动分批并使用tqdm进行批量推理 ===
    
    # --- 处理微调模型 ---
    print("开始对微调模型进行批量推理...")
    merged_outputs_list = []
    # 使用tqdm包裹分批循环
    for i in tqdm(range(0, len(merged_prompts_formatted), batch_size), desc="正在处理微调模型"):
        # 从总列表中切分出一个小批次
        batch_prompts = merged_prompts_formatted[i:i + batch_size]
        # 对这个小批次进行推理
        outputs = merged_pipe(batch_prompts, **generation_config)
        # 收集结果
        merged_outputs_list.extend(outputs)

    # --- 处理基础模型 ---
    print("开始对基础模型进行批量推理...")
    base_outputs_list = []
    # 再次使用tqdm包裹分批循环
    for i in tqdm(range(0, len(base_prompts_formatted), batch_size), desc="正在处理基础模型"):
        batch_prompts = base_prompts_formatted[i:i + batch_size]
        outputs = base_pipe(batch_prompts, **generation_config)
        base_outputs_list.extend(outputs)

    # === 6. 组合结果并保存 (逻辑微调) ===
    print("评测完成，正在组合结果并保存到CSV文件...")
    results = []
    for i in range(len(prompts_and_chosen)):
        finetuned_full_text = merged_outputs_list[i][0]['generated_text']
        finetuned_answer = finetuned_full_text.replace(merged_prompts_formatted[i], "")
        
        base_full_text = base_outputs_list[i][0]['generated_text']
        base_answer = base_full_text.replace(base_prompts_formatted[i], "")

        results.append({
            "prompt": prompts_and_chosen[i]["prompt"],
            "golden_answer": prompts_and_chosen[i]["golden_answer"],
            "finetuned_model_output": finetuned_answer,
            "base_model_output": base_answer
        })

    df = pd.DataFrame(results)
    df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')

    print(f"✅ 评测结果已成功保存至: {output_csv_path}")

if __name__ == "__main__":
    main()
```

merge_adapter.py:

```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import os

def main():
    # === 1. 配置路径 ===
    # 您的原始基础模型路径 (与训练时使用的模型一致)
    base_model_path = "Qwen2.5-7B-Instruct" 
    # 您训练好的LoRA适配器路径
    adapter_path = "Qwen2.5-7B-Instruct-sft-lora-adapter" 
    # 您希望保存合并后模型的路径
    merged_model_path = "Qwen2.5-7B-Instruct-sft-final"

    print("--- 开始合并 LoRA 适配器 ---")

    # === 2. 加载基础模型和Tokenizer ===
    print(f"正在从 '{base_model_path}' 加载基础模型...")
    # 以bfloat16精度加载，确保与训练时一致
    # 注意：这里不能使用4-bit量化 (不能有 quantization_config)
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=torch.bfloat16,
        device_map="auto", # 自动选择设备 (GPU或CPU)
    )

    print(f"正在从 '{base_model_path}' 加载Tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(base_model_path)

    # === 3. 加载并应用LoRA适配器 ===
    print(f"正在从 '{adapter_path}' 加载并应用LoRA适配器...")
    # PeftModel会自动将适配器权重加载到基础模型之上
    peft_model = PeftModel.from_pretrained(base_model, adapter_path)

    # === 4. 执行合并！ ===
    print(">>> 正在执行合并操作...")
    # merge_and_unload() 会将LoRA权重合并到基础模型中，并返回合并后的新模型
    merged_model = peft_model.merge_and_unload()
    print(">>> 合并完成！")

    # === 5. 保存合并后的完整模型和Tokenizer ===
    print(f"正在将合并后的模型保存到 '{merged_model_path}'...")
    os.makedirs(merged_model_path, exist_ok=True)
    merged_model.save_pretrained(merged_model_path)
    tokenizer.save_pretrained(merged_model_path)

    print(f"✅ 合并后的模型已成功保存至: {merged_model_path}")

if __name__ == "__main__":
    main()
```

输出结果比较奇怪，finetuned model都在输出一定的还行的内容（基本上能完成prompt中给的任务）后开始说胡话（乱码/重复前面的指令/出现幻觉如输出代码等），怀疑是chat template或什么地方没正确处理eos

使用保存多个checkpoint，看了一下训练了几个batch的模型也会有概率出现这个问题，不过训练的越靠后问题出现的概率越大。

看到GitHub上的一个issue：https://github.com/unslothai/unsloth/issues/416，刚好说的是训练完的模型没法生成eos token，导致回答完该回答的东西之后开始说重复的胡话。帖主说原因是eos token和pad token一样，这导致了模型在训练时“忽略”了eos token（被当成空白填充处理了），因此学不到应该在哪里结束。这也导致了我们感觉SFT已经在前面的文风生成部分起作用了，却让模型丢失了知道在哪里该停止的能力。而我们的模型恰好是eos_token==pad_token！应该大概率是这个问题

尝试解决：

1：使用Qwen3-4B-Instruct-2507

问题可以解决，但还是想尝试怎么在Qwen2.5上修复一下

2：我发现似乎Qwen2.5的绝大多数版本也都是`eos_token='<|im_end|>'`......恰好下载到一个不是的（大概率是Qwen2.5-7B），运气有点差。不过学到了一个坑，能通过结果分析出大概率是eos_token没学习到，也不亏吧。最后用Qwen2.5-7B-Instruct做也成功了

在Qwen2.5-7B-Instruct上跑的结果：
```
=== 评测统计报告 ===
总评测数据: 170 条

按内容类型统计:
                           best_model  golden_avg_score  finetuned_avg_score  base_avg_score
content_type
ecommerce_long_form                19              5.00                 3.89            2.93
paid_ad_bab                        19              4.69                 3.99            3.71
paid_ad_cta                        19              4.71                 4.26            3.96
paid_ad_fab                        19              4.77                 4.13            3.47
paid_ad_pas                        19              4.89                 4.02            3.39
social_media_educational           19              4.96                 3.87            2.87
social_media_myth_busting          18              4.97                 3.99            2.93
social_media_review                19              4.97                 3.82            3.04
social_media_storytelling          19              4.99                 3.74            2.89

最佳模型统计:
  A: 156 次 (91.8%)
  B: 13 次 (7.6%)
  C: 1 次 (0.6%)

整体平均分:
  Golden (标准答案): 4.88
  Finetuned (微调模型): 3.97
  Base (基础模型): 3.25
```