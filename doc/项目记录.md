# 记录

本文档为项目的记录文档

## 项目启动

本项目旨在使用SFT+GRPO训练一个LLM，帮助“原素方程”品牌构建一个品牌专属的、能够自动化生成高质量营销文案的AI助手。侧重于社交媒体笔记文案、电商描述文案和付费广告文案。

**原素方程 品牌风格指南**

- 品牌理念: 

精简有效，回归肌肤本源。我们专注于科学配方，摒弃不必要的成分堆砌。

- 品牌人格: 

一位冷静、专业、值得信赖的皮肤科医生或科研人员。

- 沟通要点:

专业严谨: 用词准确，逻辑清晰，像在做一次小科普。

冷静客观: 专注于解决方案，不贩卖焦虑，避免使用夸张的感叹句。

透明真诚: 清晰解释核心成分与功效，不使用模糊、神秘的词汇。

- 关键词选用:

鼓励使用: 屏障, 修护, 科学, 精简, 配方, 浓度, 源头, 温和。

坚决避免: 神器, 逆天, 奇迹, 秒杀, 绝绝子, yyds, 家人们。

## 大致思路

对给定产品列表，划分训练集、验证集；使用DeepSeek-V3构造特定方向文案，多轮清洗直到符合要求；混合使用多种策略构造Reward Model需要的rejected数据；训练SFT model, reward model, GRPO model。使用LLM as a Judge来评测模型效果。

## 过程记录

1. 前期准备&首次生成：对于给定的产品列表，划分训练集、验证集，每种产品分别构造3大类文案，分为9小类。使用的脚本为`dataset_generate/generate_sft_datasets.py`，使用的prompts位于`sft_gen_prompts.py`。生成`data/sft_dataset_deepseek.jsonl`和`data/val_dataset_deepseek.jsonl`(2025.8.20)
2. 数据清洗：对于首次生成的数据集，存在问题：1. 有不知道为什么会加进去的标签 2. 有的社交媒体笔记过于平淡，缺少人设真实感和故事感。针对这两点进行优化，使用`dataset_generate/sft_clean_prompts.py`的prompt和`dataset_generate/clean_sft_datasets.py`脚本，对数据进行优化，产物为`sft_dataset_cleaned.jsonl`。(2025.8.22)后续又发现部分广告文案缺少最后的号召部分，又使用`dataset_generate/clean_sft_datasets_cta.py`脚本和`dataset_generate/sft_clean_prompts_cta.py`的prompt进行清洗，添加CTA部分，产物为`data/sft_dataset_cleaned_cta.jsonl`。(2025.8.31)
3. 构造Benchmark：使用**LLM as a Jugde**，针对三种类型的侧重点构造prompt，位于`judge/judge_prompts.py`。使用大模型，对三种模型在验证集上的输出进行打分。(2025.8.30)
4. 构造奖励模型所需数据：正样本沿用为sft构造的数据。负样本使用50%“削弱文案”（使用弱prompt+弱模型生成方向接近但质量不高的文案）+30%“夸张文案”（故意添加夸大言辞的语料，违背品牌原则）+20%“添加错误”（添加1-2个不实信息）来构造。通过构造这样的负样本，可以让奖励模型对文案文笔不够好、违背品牌初衷和错误的信息进行惩罚。具体的prompts位于`dataset_generate/rm_gen_rej_prompts_exaggerated.py`, `dataset_generate/rm_gen_rej_prompts_fake.py`, `dataset_generate/rm_gen_rej_prompts_weak.py`，脚本为`generate_rm_datasets.py` (2025.8.30)
   由于原始sft_dataset又清洗了（添加了CTA），又将新内容更新到`rm_dataset_updated.csv`（添加CTA后的内容）(2025.8.31)
5. 转换为最终数据格式：从`data/rm_dataset_final.jsonl`转换成`data/sft_dataset_final.jsonl`，换成了hugging face trl库支持的格式(2025.9.1)；
6. SFT训练：采用LoRA，训练模型脚本`sft_trainer.py`，LoRA Adapter合并脚本`merge_adapter.py`，测试验证脚本`sft_tester.py`。(2025.9.1)



2025.9.1 本地测试版(0.5B模型)SFT结果跑出来了。其中golden是我们预先准备的高质量数据，在这里作为参考
```
=== 评测统计报告 ===
总评测数据: 170 条

按内容类型统计:
                           best_model  golden_avg_score  finetuned_avg_score  base_avg_score
content_type
ecommerce_long_form                19              5.00                 3.21            2.51
paid_ad_bab                        19              4.61                 2.54            3.16
paid_ad_cta                        19              4.51                 2.96            3.75
paid_ad_fab                        19              4.33                 3.02            4.05
paid_ad_pas                        19              4.77                 2.47            3.89
social_media_educational           19              4.91                 3.04            1.91
social_media_myth_busting          18              4.98                 2.81            2.07
social_media_review                19              4.98                 3.23            2.32
social_media_storytelling          19              4.98                 2.98            2.14

最佳模型统计:
  A: 145 次 (85.3%)
  C: 23 次 (13.5%)
  B: 2 次 (1.2%)

整体平均分:
  Golden (标准答案): 4.79
  Finetuned (微调模型): 2.92
  Base (基础模型): 2.87
```

问题：微调完模型的social_media类和ecommence类都比base model强，但paid_ad都弱。

观察发现在paid_ad上，我们预先准备的黄金数据得分也稍微低了一点，甚至有的和0.5B的base model产出的非常接近。但观察发现，其实二者天差地别，例如：

golden data: 
```
你的皮肤是否因氧化而暗沉？原素方程绿茶抗氧面膜，科学配方精简有效。绿茶提取物+麦角硫因协同抗氧化，从源头改善熬夜脸，温和提亮肤色。点击立即体验科学护肤的力量。
```

base model: 
```
洞见肌肤真谛，天然守护新世代】—— 选择我们的“原素方程”品牌，您将开启一段全新的护肤旅程！在这款“绿茶抗氧面膜”中，我们为您精心挑选了三重守护魔法：强效抗氧化、改善熬夜脸，以及快速提亮肌肤。

【核心原料】
- 绿茶提取物：富含丰富的抗氧化成分，能够帮助抵抗环境中的自由基损伤。
- 麦角硫因：有效促进胶原蛋白生成，提升皮肤弹性和光泽度。
- 维生素E：强大的抗氧化剂，能对抗自由基，让肌肤焕发自然光彩。
- 透明质酸：深层补水保湿，滋润干燥敏感性肌肤。

【多重功效】
1. 强效抗氧化：绿茶和麦角硫因共同作用，不仅增强肌肤对自由基的抵御能力，还能显著减少细纹和皱纹的产生。
2. 改善熬夜脸：含有维生素E和麦角硫因的配方，可促进皮脂腺分泌，加速细胞更新，修复受损的皮肤屏障，使肌肤恢复光泽。
3. 快速提亮：使用后肌肤会逐渐回弹，提亮肤色，展现出更加健康、年轻、活力充沛的外观。

【独特优势】
我们的“绿茶抗氧面膜”不同于传统护肤品，它采用的是天然植物精华，没有添加任何化学添加剂。因此，它对人体无害，适合所有肤质，特别适合肤色暗沉者。

【行动号召】
立即行动，加入我们的绿色护肤行列，让肌肤成为您的首选，享受更健康的美丽之旅！

【链接链接】
如果您还在等待或渴望一个安静、舒适的清晨，那么现在加入“原素方程”的大家庭吧！我们的“绿茶抗氧面膜”将为您提供前所未有的健康与活力。

---

这是一条精心设计的产品广告文案，旨在强调其产品的多重功效和独特的理念。通过引用关键元素并突出其独特的配方和护肤特点，吸引目标消费者进行购买行动。
```

而评测模型认为base model更好。考虑是不是评测标准定的不好。尝试优化了一下paid_ad相关prompts，最佳模型是A的情况更多了，更合理了

第二版结果：
```
=== 评测统计报告 ===

总评测数据: 170 条

按内容类型统计:

content_type               best_model  golden_avg_score  finetuned_avg_score  base_avg_score

ecommerce_long_form                19              5.00                 3.19            2.44
paid_ad_bab                        19              4.83                 2.53            3.39
paid_ad_cta                        19              4.71                 3.01            3.78
paid_ad_fab                        19              4.68                 3.12            3.61
paid_ad_pas                        19              4.87                 2.47            3.72
social_media_educational           19              4.91                 3.07            2.12
social_media_myth_busting          18              5.00                 2.76            2.19
social_media_review                19              4.98                 3.19            2.19
social_media_storytelling          19              4.98                 2.98            2.07


最佳模型统计:

  A: 162 次 (95.3%)
  C: 6 次 (3.5%)
  B: 2 次 (1.2%)

整体平均分:

  Golden (标准答案): 4.88

  Finetuned (微调模型): 2.93

  Base (基础模型): 2.84
```

又微调了一版本的评测prompt：
```
=== 评测统计报告 ===
总评测数据: 170 条

按内容类型统计:
                           best_model  golden_avg_score  finetuned_avg_score  base_avg_score
content_type
ecommerce_long_form                19              5.00                 3.18            2.28
paid_ad_bab                        19              4.80                 2.42            3.39
paid_ad_cta                        19              4.74                 2.83            3.67
paid_ad_fab                        19              4.70                 2.75            3.41
paid_ad_pas                        19              4.95                 2.28            3.76
social_media_educational           19              4.89                 2.96            1.82
social_media_myth_busting          18              5.00                 2.72            1.87
social_media_review                19              5.00                 3.09            1.75
social_media_storytelling          19              4.98                 2.96            1.89

最佳模型统计:
  A: 167 次 (98.2%)
  C: 2 次 (1.2%)
  B: 1 次 (0.6%)

整体平均分:
  Golden (标准答案): 4.90
  Finetuned (微调模型): 2.80
  Base (基础模型): 2.66
```

变化不大，感觉有的地方评测还是不完全准，但好歹大方向差不多了。

2025.9.2 对社交媒体文案和广告文案添加“指令遵循度”指标，效果如下：
```
=== 评测统计报告 ===
总评测数据: 170 条

按内容类型统计:
                           best_model  golden_avg_score  finetuned_avg_score  base_avg_score
content_type
ecommerce_long_form                19              5.00                 3.11            2.16
paid_ad_bab                        19              4.92                 2.29            3.32
paid_ad_cta                        19              4.89                 2.68            3.35
paid_ad_fab                        19              4.83                 2.86            3.39
paid_ad_pas                        19              4.99                 2.25            3.66
social_media_educational           19              4.99                 2.97            1.75
social_media_myth_busting          18              5.00                 2.69            2.08
social_media_review                19              4.99                 3.16            2.11
social_media_storytelling          19              5.00                 2.99            2.01

最佳模型统计:
  A: 170 次 (100.0%)

整体平均分:
  Golden (标准答案): 4.96
  Finetuned (微调模型): 2.78
  Base (基础模型): 2.65
```



### SFT 模型训练记录

环境在

```bash
conda activate /root/autodl-tmp/train/adginus_env
```

- 第一版

train.py

```python
import torch
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig
from trl import SFTTrainer, SFTConfig
import os

def main():
    # === 1. 配置模型、数据和输出路径 ===
    # 模型ID
    base_model_name = "Qwen2.5-7B-Instruct"
    # 预处理好的数据集路径
    dataset_path = "train/data/sft_dataset_final.jsonl"
    # LoRA适配器输出目录
    output_dir = "Qwen2.5-7B-Instruct-sft-lora-adapter-2"

    # === 2. 加载数据集 ===
    print(f"正在从 '{dataset_path}' 加载数据集...")
    # 确保文件存在，否则创建一个示例
    if not os.path.exists(dataset_path):
        print(f"错误: 数据集文件 '{dataset_path}' 不存在。请先运行 preprocess_sft_data.py。")
        return
    dataset = load_dataset("json", data_files=dataset_path, split="train")
    print(f"数据集长度: {len(dataset)}")

    # === 3. 配置LoRA (PEFT) ===
    # 这是官方文档推荐的与SFTTrainer结合使用的方法
    print(">>> 正在配置LoRA...")
    peft_config = LoraConfig(
        r=64,
        lora_alpha=128,
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    )

    # === 4. 配置训练参数 (严格使用SFTConfig) ===
    print(">>> 正在配置SFTConfig训练参数...")
    training_args = SFTConfig(
        # --- 核心训练参数 ---
        output_dir=output_dir,
        num_train_epochs=1,
        per_device_train_batch_size=1,  # 由于显存占用大，从1开始
        gradient_accumulation_steps=8,  # 等效batch_size = 1 * 8 = 8
        learning_rate=1e-5,
        lr_scheduler_type="cosine",
        # optim="paged_adamw_8bit", # 依然推荐，可以节省一些显存

        # --- 模型加载参数 (通过model_init_kwargs传递) ---
        # 这是官方文档推荐的加载方式，而不是在外部加载模型
        model_init_kwargs={"torch_dtype": torch.bfloat16, "device_map": "auto"},

        # --- SFTTrainer特有参数 ---
        max_length=2048,
        packing=True, # 启用packing提升效率

        # --- 日志和保存 ---
        logging_steps=20,
        save_strategy="epoch",
        
        # --- 精度相关 ---
        bf16=True, # 在支持的硬件上启用bf16
    )

    # === 5. 初始化并开始训练 ===
    print(">>> 正在初始化SFT Trainer...")
    # 注意：现在我们将模型ID（字符串）直接传给Trainer
    # Trainer会使用SFTConfig中的model_init_kwargs来加载模型
    trainer = SFTTrainer(
        model=base_model_name,
        args=training_args,
        train_dataset=dataset,
        # tokenizer在未提供时，会自动从模型ID加载
        peft_config=peft_config,
        # dataset_text_field="messages", # 当列名为'messages'时，trainer会自动识别，通常无需指定
    )
    
    # 检查并设置pad_token (一个好的实践)
    if trainer.tokenizer.pad_token is None:
        trainer.tokenizer.pad_token = trainer.tokenizer.eos_token

    print(">>> 开始训练...")
    trainer.train()

    print(">>> 训练完成，正在保存最终的LoRA适配器...")
    trainer.save_model(output_dir)

    print(f"✅ LoRA适配器已成功保存至: {output_dir}")

if __name__ == "__main__":
    main()
```

merge_adapter.py:

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import os

def main():
    # === 1. 配置路径 ===
    # 您的原始基础模型路径 (与训练时使用的模型一致)
    base_model_path = "Qwen2.5-7B-Instruct" 
    # 您训练好的LoRA适配器路径
    adapter_path = "Qwen2.5-7B-Instruct-sft-lora-adapter" 
    # 您希望保存合并后模型的路径
    merged_model_path = "Qwen2.5-7B-Instruct-sft-final"

    print("--- 开始合并 LoRA 适配器 ---")

    # === 2. 加载基础模型和Tokenizer ===
    print(f"正在从 '{base_model_path}' 加载基础模型...")
    # 以bfloat16精度加载，确保与训练时一致
    # 注意：这里不能使用4-bit量化 (不能有 quantization_config)
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=torch.bfloat16,
        device_map="auto", # 自动选择设备 (GPU或CPU)
    )

    print(f"正在从 '{base_model_path}' 加载Tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(base_model_path)

    # === 3. 加载并应用LoRA适配器 ===
    print(f"正在从 '{adapter_path}' 加载并应用LoRA适配器...")
    # PeftModel会自动将适配器权重加载到基础模型之上
    peft_model = PeftModel.from_pretrained(base_model, adapter_path)

    # === 4. 执行合并！ ===
    print(">>> 正在执行合并操作...")
    # merge_and_unload() 会将LoRA权重合并到基础模型中，并返回合并后的新模型
    merged_model = peft_model.merge_and_unload()
    print(">>> 合并完成！")

    # === 5. 保存合并后的完整模型和Tokenizer ===
    print(f"正在将合并后的模型保存到 '{merged_model_path}'...")
    os.makedirs(merged_model_path, exist_ok=True)
    merged_model.save_pretrained(merged_model_path)
    tokenizer.save_pretrained(merged_model_path)

    print(f"✅ 合并后的模型已成功保存至: {merged_model_path}")

if __name__ == "__main__":
    main()
```

test.py

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from datasets import load_dataset
import pandas as pd
from tqdm import tqdm

def main():
    # === 1. 配置路径和参数 ===
    merged_model_path = "Qwen2.5-7B-Instruct-sft-final"
    base_model_path = "Qwen2.5-7B-Instruct"
    test_data_path = "train/data/val_dataset_final.jsonl"
    output_csv_path = "train/data/evaluation_results_final.csv"

    generation_config = {
        "max_new_tokens": 512,
        "do_sample": True,
        "temperature": 0.1,
        "top_k": 50,
        "top_p": 0.95,
    }

    batch_size = 8
    print("--- 开始模型对比评测 (优化版 + TQDM进度条) ---")

    # === 2. 加载模型和Tokenizer (已修复padding问题) ===
    print(f"正在加载微调模型: {merged_model_path}")
    merged_model = AutoModelForCausalLM.from_pretrained(merged_model_path, device_map="auto", dtype="auto")
    merged_tokenizer = AutoTokenizer.from_pretrained(merged_model_path, padding_side='left')

    print(f"正在加载基础模型: {base_model_path}")
    base_model = AutoModelForCausalLM.from_pretrained(base_model_path, device_map="auto", dtype="auto")
    base_tokenizer = AutoTokenizer.from_pretrained(base_model_path, padding_side='left')

    if merged_tokenizer.pad_token is None:
        merged_tokenizer.pad_token = merged_tokenizer.eos_token
    if base_tokenizer.pad_token is None:
        base_tokenizer.pad_token = base_tokenizer.eos_token

    # === 3. 创建Pipeline用于推理 ===
    merged_pipe = pipeline("text-generation", model=merged_model, tokenizer=merged_tokenizer)
    base_pipe = pipeline("text-generation", model=base_model, tokenizer=base_tokenizer)

    # === 4. 加载并准备所有prompts (无变化) ===
    print(f"正在加载并准备所有prompts...")
    test_dataset = load_dataset("json", data_files=test_data_path, split="train")
    
    prompts_and_chosen = []
    merged_prompts_formatted = []
    base_prompts_formatted = []
    for sample in test_dataset:
        prompt_text = sample["prompt"]
        chosen_text = sample["chosen"]
        messages = [{"role": "user", "content": prompt_text}]
        prompts_and_chosen.append({"prompt": prompt_text, "golden_answer": chosen_text})
        merged_prompts_formatted.append(merged_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))
        base_prompts_formatted.append(base_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))

    # === 5. --- MODIFIED --- 手动分批并使用tqdm进行批量推理 ===
    
    # --- 处理微调模型 ---
    print("开始对微调模型进行批量推理...")
    merged_outputs_list = []
    # 使用tqdm包裹分批循环
    for i in tqdm(range(0, len(merged_prompts_formatted), batch_size), desc="正在处理微调模型"):
        # 从总列表中切分出一个小批次
        batch_prompts = merged_prompts_formatted[i:i + batch_size]
        # 对这个小批次进行推理
        outputs = merged_pipe(batch_prompts, **generation_config)
        # 收集结果
        merged_outputs_list.extend(outputs)

    # --- 处理基础模型 ---
    print("开始对基础模型进行批量推理...")
    base_outputs_list = []
    # 再次使用tqdm包裹分批循环
    for i in tqdm(range(0, len(base_prompts_formatted), batch_size), desc="正在处理基础模型"):
        batch_prompts = base_prompts_formatted[i:i + batch_size]
        outputs = base_pipe(batch_prompts, **generation_config)
        base_outputs_list.extend(outputs)

    # === 6. 组合结果并保存 (逻辑微调) ===
    print("评测完成，正在组合结果并保存到CSV文件...")
    results = []
    for i in range(len(prompts_and_chosen)):
        finetuned_full_text = merged_outputs_list[i][0]['generated_text']
        finetuned_answer = finetuned_full_text.replace(merged_prompts_formatted[i], "")
        
        base_full_text = base_outputs_list[i][0]['generated_text']
        base_answer = base_full_text.replace(base_prompts_formatted[i], "")

        results.append({
            "prompt": prompts_and_chosen[i]["prompt"],
            "golden_answer": prompts_and_chosen[i]["golden_answer"],
            "finetuned_model_output": finetuned_answer,
            "base_model_output": base_answer
        })

    df = pd.DataFrame(results)
    df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')

    print(f"✅ 评测结果已成功保存至: {output_csv_path}")

if __name__ == "__main__":
    main()
```

输出结果比较奇怪，finetuned model都在输出一定的还行的内容（基本上能完成prompt中给的任务）后开始说胡话（乱码/重复前面的指令/出现幻觉如输出代码等），怀疑是chat template或什么地方没正确处理eos

使用保存多个checkpoint，看了一下训练了几个batch的模型也会有概率出现这个问题，不过训练的越靠后问题出现的概率越大。

看到GitHub上的一个issue：https://github.com/unslothai/unsloth/issues/416，刚好说的是训练完的模型没法生成eos token，导致回答完该回答的东西之后开始说重复的胡话。帖主说原因是eos token和pad token一样，这导致了模型在训练时“忽略”了eos token（被当成空白填充处理了），因此学不到应该在哪里结束。这也导致了我们感觉SFT已经在前面的文风生成部分起作用了，却让模型丢失了知道在哪里该停止的能力。而我们的模型恰好是eos_token==pad_token！应该大概率是这个问题

尝试解决：

1：使用Qwen3-4B-Instruct-2507

问题可以解决，但还是想尝试怎么在Qwen2.5上修复一下

2：我发现似乎Qwen2.5的绝大多数版本也都是`eos_token='<|im_end|>'`......恰好下载到一个不是的（大概率是Qwen2.5-7B），运气有点差。不过学到了一个坑，能通过结果分析出大概率是eos_token没学习到，也不亏吧。最后用Qwen2.5-7B-Instruct做也成功了

- 在Qwen2.5-7B-Instruct和其SFT模型上跑的结果：

```
=== 评测统计报告 ===
总评测数据: 170 条

按内容类型统计:
                           best_model  golden_avg_score  finetuned_avg_score  base_avg_score
content_type
ecommerce_long_form                19              5.00                 3.89            2.93
paid_ad_bab                        19              4.69                 3.99            3.71
paid_ad_cta                        19              4.71                 4.26            3.96
paid_ad_fab                        19              4.77                 4.13            3.47
paid_ad_pas                        19              4.89                 4.02            3.39
social_media_educational           19              4.96                 3.87            2.87
social_media_myth_busting          18              4.97                 3.99            2.93
social_media_review                19              4.97                 3.82            3.04
social_media_storytelling          19              4.99                 3.74            2.89

最佳模型统计:
  A: 156 次 (91.8%)
  B: 13 次 (7.6%)
  C: 1 次 (0.6%)

整体平均分:
  Golden (标准答案): 4.88
  Finetuned (微调模型): 3.97
  Base (基础模型): 3.25
```

有效果了，只是偶尔还会有反复输出，可能是模型参数量不够大，能力不够

2025.9.3 改过judge_new之后再评测：

- 对于golden_response:
```
=== 评测统计报告 ===
总评测数据: 170 条

按内容类型统计:
                          avg_score      
                              count  mean
content_type
ecommerce_long_form              19  4.96
paid_ad_bab                      19  4.81
paid_ad_cta                      19  4.83
paid_ad_fab                      19  4.75
paid_ad_pas                      19  4.93
social_media_educational         19  4.75
social_media_myth_busting        18  4.78
social_media_review              19  4.83
social_media_storytelling        19  4.91

整体平均分: 4.84

各维度平均分:
  score_authenticity_immersion: 4.35
  score_value_engagement: 4.92
  score_brand_fit: 5.00
  score_instruction_following: 5.00
  score_professionalism_persuasion: 4.95
  score_clarity_value: 5.00
  score_trust_building: 4.95
  score_suitability_brevity: 4.88
  score_click_appeal: 4.39
  score_cta_effectiveness: 4.87
  score_factual_accuracy: 5.00
```

- 对于base_model Qwen2.5-7B-Instruct:
```
总评测数据: 170 条

按内容类型统计:
                          avg_score
                              count  mean
content_type
ecommerce_long_form              19  4.28
paid_ad_bab                      19  4.21
paid_ad_cta                      19  4.60
paid_ad_fab                      19  4.06
paid_ad_pas                      19  4.21
social_media_educational         19  3.63
social_media_myth_busting        18  3.86
social_media_review              19  3.75
social_media_storytelling        19  3.74

整体平均分: 4.04

各维度平均分:
  score_authenticity_immersion: 3.21
  score_value_engagement: 3.76
  score_brand_fit: 3.21
  score_instruction_following: 4.89
  score_professionalism_persuasion: 4.00
  score_clarity_value: 4.89
  score_trust_building: 3.95
  score_suitability_brevity: 3.25
  score_click_appeal: 3.88
  score_cta_effectiveness: 4.22
  score_factual_accuracy: 5.00
```

- 对于Finetuned Qwen2.5-7B-Instruct:
```
=== 评测统计报告 ===
总评测数据: 170 条

按内容类型统计:
                          avg_score
                              count  mean
content_type
ecommerce_long_form              19  4.58
paid_ad_bab                      19  4.17
paid_ad_cta                      19  4.75
paid_ad_fab                      19  4.05
paid_ad_pas                      19  4.23
social_media_educational         19  4.47
social_media_myth_busting        18  4.61
social_media_review              19  4.28
social_media_storytelling        19  4.45

整体平均分: 4.40

各维度平均分:
  score_authenticity_immersion: 3.79
  score_value_engagement: 4.39
  score_brand_fit: 4.65
  score_instruction_following: 4.91
  score_professionalism_persuasion: 4.32
  score_clarity_value: 5.00
  score_trust_building: 4.42
  score_suitability_brevity: 3.88
  score_click_appeal: 4.03
  score_cta_effectiveness: 3.74
  score_factual_accuracy: 5.00

评测结果已保存到: results/finetuned_model_results.csv
```
看出来给分区分度变小了，可能还是prompt里打分标准太仁慈了

**2025.9.3：**

- 在Qwen3-4B-Instruct-2507上的结果：

```
=== 评测统计报告 ===
总评测数据: 170 条

按内容类型统计:
                           best_model  golden_avg_score  finetuned_avg_score  base_avg_score
content_type
ecommerce_long_form                19              4.72                 4.26            3.02
paid_ad_bab                        19              4.72                 3.84            3.85
paid_ad_cta                        19              4.40                 4.64            4.16
paid_ad_fab                        19              4.56                 4.22            4.01
paid_ad_pas                        19              4.68                 4.33            4.09
social_media_educational           19              4.84                 4.01            3.97
social_media_myth_busting          18              4.88                 3.82            4.12
social_media_review                19              4.87                 4.07            3.46
social_media_storytelling          19              4.95                 3.91            3.55

最佳模型统计:
  A: 128 次 (75.3%)
  B: 30 次 (17.6%)
  C: 12 次 (7.1%)

整体平均分:
  Golden (标准答案): 4.73
  Finetuned (微调模型): 4.12
  Base (基础模型): 3.80
```

能看出Qwen3-4B-Instruct-2507比Qwen2.5-7B-Instruct强了不少，不微调的版本得分都变高了。

**更新new_judge.py后的效果：**

- 对于base model Qwen3-4B-Instruct:
```
=== 评测统计报告 ===
总评测数据: 170 条

按内容类型统计:
                          avg_score
                              count  mean
content_type
ecommerce_long_form              19  4.82
paid_ad_bab                      19  4.21
paid_ad_cta                      19  4.96
paid_ad_fab                      19  4.33
paid_ad_pas                      19  4.62
social_media_educational         19  4.57
social_media_myth_busting        18  4.62
social_media_review              19  4.57
social_media_storytelling        19  4.47

整体平均分: 4.57

各维度平均分:
  score_authenticity_immersion: 4.08
  score_value_engagement: 4.92
  score_brand_fit: 4.23
  score_instruction_following: 4.99
  score_professionalism_persuasion: 4.74
  score_clarity_value: 5.00
  score_trust_building: 4.74
  score_suitability_brevity: 3.63
  score_click_appeal: 4.49
  score_cta_effectiveness: 4.55
  score_factual_accuracy: 5.00
```

- 微调后Qwen3-4B-Instruct的效果：
```
=== 评测统计报告 ===
总评测数据: 170 条

按内容类型统计:
                          avg_score
                              count  mean
content_type
ecommerce_long_form              19  4.93
paid_ad_bab                      19  4.53
paid_ad_cta                      19  4.94
paid_ad_fab                      19  4.44
paid_ad_pas                      19  4.78
social_media_educational         19  4.71
social_media_myth_busting        18  4.67
social_media_review              19  4.46
social_media_storytelling        19  4.62

整体平均分: 4.67

各维度平均分:
  score_authenticity_immersion: 4.11
  score_value_engagement: 4.57
  score_brand_fit: 4.80
  score_instruction_following: 4.99
  score_professionalism_persuasion: 4.89
  score_clarity_value: 5.00
  score_trust_building: 4.89
  score_suitability_brevity: 4.71
  score_click_appeal: 4.53
  score_cta_effectiveness: 4.12
  score_factual_accuracy: 5.00
```



从AutoDL服务器上下载模型的方法：

参考[autodl批量下载文件_autodl下载文件到本地-CSDN博客](https://blog.csdn.net/qq_43390229/article/details/146999502)方法2，用AutoPanel-存到阿里云盘再下载到本地。直接从AutoDL上下载速度只有几百k



### Reward Model 训练记录

**2025.9.3**

发现奖励模型也需要验证集，于是把val_dataset_cleaned.jsonl使用generate_rm_datasets.py脚本生成了rejected样例。然后合并到val_dataset_final.jsonl里去了。前面用的val_dataset_final.jsonl光荣退役，变成val_dataset_final_deprecated.jsonl

训练结果：
```
=== 开始奖励模型微调 ===

--- 1. 准备数据集中... ---
成功读取 612 条训练数据
成功读取 170 条验证数据
--- 数据集准备完成 ---

--- 2. 加载基础模型: Skywork-Reward-V2-Qwen3-4B ---
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.04s/it]
--- 模型加载成功，将运行在: cuda ---

--- 3. 数据预处理中... ---
Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 612/612 [00:00<00:00, 1020.84 examples/s]
Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 170/170 [00:00<00:00, 1199.78 examples/s]
--- 数据预处理完成 ---

--- 4. 配置训练参数并开始训练... ---
Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
开始训练...
  0%|                                                                                                                                                                           | 0/306 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
{'loss': 0.9711, 'grad_norm': 7.979763507843018, 'learning_rate': 9.000000000000001e-07, 'epoch': 0.03}                                                                                                 
{'loss': 0.0924, 'grad_norm': 0.161222904920578, 'learning_rate': 1.9000000000000002e-06, 'epoch': 0.07}                                                                                                
{'loss': 0.0884, 'grad_norm': 1.1843732661276363e-09, 'learning_rate': 2.9e-06, 'epoch': 0.1}                                                                                                           
{'loss': 0.0347, 'grad_norm': 78.54852294921875, 'learning_rate': 3.900000000000001e-06, 'epoch': 0.13}                                                                                                 
{'loss': 0.0831, 'grad_norm': 0.007094039116054773, 'learning_rate': 4.9000000000000005e-06, 'epoch': 0.16}                                                                                             
{'loss': 0.005, 'grad_norm': 1.6191341956073302e-06, 'learning_rate': 4.824218750000001e-06, 'epoch': 0.2}                                                                                              
{'loss': 0.0288, 'grad_norm': 1.021157290670427e-11, 'learning_rate': 4.62890625e-06, 'epoch': 0.23}                                                                                                    
{'loss': 0.0534, 'grad_norm': 1.6262794133581338e-07, 'learning_rate': 4.433593750000001e-06, 'epoch': 0.26}                                                                                            
{'loss': 0.0, 'grad_norm': 3.4824150263760775e-14, 'learning_rate': 4.23828125e-06, 'epoch': 0.29}                                                                                                      
{'loss': 0.0007, 'grad_norm': 0.4855482280254364, 'learning_rate': 4.0429687500000004e-06, 'epoch': 0.33}                                                                                               
{'loss': 0.0012, 'grad_norm': 2.486711357663207e-12, 'learning_rate': 3.84765625e-06, 'epoch': 0.36}                                                                                                    
{'loss': 0.0347, 'grad_norm': 0.0, 'learning_rate': 3.6523437500000003e-06, 'epoch': 0.39}                                                                                                              
{'loss': 0.0, 'grad_norm': 2.1800346367324024e-16, 'learning_rate': 3.45703125e-06, 'epoch': 0.42}                                                                                                      
{'loss': 0.0, 'grad_norm': 7.963362254161175e-16, 'learning_rate': 3.26171875e-06, 'epoch': 0.46}                                                                                                       
{'loss': 0.0262, 'grad_norm': 0.0, 'learning_rate': 3.0664062500000004e-06, 'epoch': 0.49}                                                                                                              
{'loss': 0.0, 'grad_norm': 1.806079730215515e-08, 'learning_rate': 2.8710937500000003e-06, 'epoch': 0.52}                                                                                               
{'loss': 0.0, 'grad_norm': 1.663379478600291e-08, 'learning_rate': 2.6757812500000002e-06, 'epoch': 0.56}                                                                                               
{'loss': 0.0006, 'grad_norm': 0.0, 'learning_rate': 2.48046875e-06, 'epoch': 0.59}                                                                                                                      
{'loss': 0.1378, 'grad_norm': 1.3493537071116585e-13, 'learning_rate': 2.28515625e-06, 'epoch': 0.62}                                                                                                   
{'loss': 0.0145, 'grad_norm': 2.961201984370853e-15, 'learning_rate': 2.08984375e-06, 'epoch': 0.65}                                                                                                    
{'loss': 0.0, 'grad_norm': 4.946139851857989e-12, 'learning_rate': 1.89453125e-06, 'epoch': 0.69}                                                                                                       
{'loss': 0.0157, 'grad_norm': 5.857411000675938e-09, 'learning_rate': 1.6992187500000002e-06, 'epoch': 0.72}                                                                                            
{'loss': 0.0, 'grad_norm': 3.071690812816996e-08, 'learning_rate': 1.5039062500000001e-06, 'epoch': 0.75}                                                                                               
{'loss': 0.0, 'grad_norm': 3.12852222350557e-07, 'learning_rate': 1.30859375e-06, 'epoch': 0.78}                                                                                                        
{'loss': 0.0009, 'grad_norm': 1.061596890394867e-06, 'learning_rate': 1.1132812500000002e-06, 'epoch': 0.82}                                                                                            
{'loss': 0.0634, 'grad_norm': 0.4010314643383026, 'learning_rate': 9.179687500000001e-07, 'epoch': 0.85}                                                                                                
{'loss': 0.0, 'grad_norm': 1.2882578559469948e-08, 'learning_rate': 7.226562500000001e-07, 'epoch': 0.88}                                                                                               
{'loss': 0.0214, 'grad_norm': 1.5217898155127942e-17, 'learning_rate': 5.2734375e-07, 'epoch': 0.92}                                                                                                    
{'loss': 0.0547, 'grad_norm': 1.0430891634666022e-19, 'learning_rate': 3.3203125e-07, 'epoch': 0.95}                                                                                                    
{'loss': 0.0, 'grad_norm': 1.447696346482119e-12, 'learning_rate': 1.3671875000000001e-07, 'epoch': 0.98}                                                                                               
{'train_runtime': 1348.9067, 'train_samples_per_second': 0.454, 'train_steps_per_second': 0.227, 'train_loss': 0.056489772882737335, 'epoch': 1.0}                                                      
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 306/306 [22:28<00:00,  4.41s/it]
--- 训练完成！---

--- 5. 保存微调后的模型... ---
--- 模型已成功保存到 'Skywork-Reward-V2-Qwen3-4B-finetuned' 目录 ---
--- 训练配置信息已保存 ---
=== 奖励模型微调完成 ===

（log应该是健康的）
```

测试结果：
```
=== 奖励模型对比测试开始 ===
正在加载验证数据集...
成功加载验证数据集: 170 条

正在加载基础模型...
--- 正在加载奖励模型: Skywork-Reward-V2-Qwen3-4B ---
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.34s/it]
--- 模型加载成功，运行在: cuda ---

正在加载微调后模型...
--- 正在加载奖励模型: Skywork-Reward-V2-Qwen3-4B-finetuned ---
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 69.72it/s]
--- 模型加载成功，运行在: cuda ---

--- 开始评估 基础模型 ---
已处理 50/170 条数据
已处理 100/170 条数据
已处理 150/170 条数据
基础模型 评估完成:
  正样本平均分数: 17.8422
  负样本平均分数: 12.4769
  分数差异: 5.3653

--- 开始评估 微调后模型 ---
已处理 50/170 条数据
已处理 100/170 条数据
已处理 150/170 条数据
微调后模型 评估完成:
  正样本平均分数: 2.2866
  负样本平均分数: -41.8639
  分数差异: 44.1505

============================================================
模型对比结果
============================================================
数据集大小: 170 条

基础模型:
  正样本平均分数: 17.8422
  负样本平均分数: 12.4769
  分数差异: 5.3653

微调后模型:
  正样本平均分数: 2.2866
  负样本平均分数: -41.8639
  分数差异: 44.1505

改进情况:
  正样本分数变化: -15.5556
  负样本分数变化: -54.3408
  分数差异变化: +38.7852
  ✅ 微调后模型表现更好，分数差异增加了 38.7852
=== 奖励模型对比测试完成 ===
```

### GRPO 训练记录

使用VeRL训练，因此需要准备parquet格式数据集。写两个脚本sft2parquet.py和val2parquet.py，得到sft_dataset_final.parquet和val_dataset_final.parquet

环境位于
```bash
conda activate /root/autodl-tmp/verl-env
```

安装verl所需环境（在verl根目录下）：
```bash
USE_MEGATRON=0 bash scripts/install_vllm_sglang_mcore.sh

pip install --no-deps -e .
```

这次没之前跑demo那次那么幸运，在一个地方直接速度极其缓慢：
```
Collecting flashinfer_python==0.2.3 (from sglang==0.4.6.post1->sglang[all]==0.4.6.post1)

  Downloading https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.3/flashinfer_python-0.2.3%2Bcu124torch2.6-cp38-abi3-linux_x86_64.whl (542.4 MB)

     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/542.4 MB 18.9 kB/s eta 7:55:47
```

直接把这个链接里的东西下载后pip install:

```
(/root/autodl-tmp/verl-env) root@autodl-container-c6d54aa471-1d603b3b:~/autodl-tmp/verl# pip install flashinfer_python-0.2.3+cu124torch2.6-cp38-abi3-linux_x86_64.whl
```

verl安装脚本里还有两个速度可能会卡死的大户：

```
echo "3. install FlashAttention and FlashInfer"
# Install flash-attn-2.7.4.post1 (cxx11abi=False)
wget -nv https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp310-cp310-linux_x86_64.whl && \
    pip install --no-cache-dir flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp310-cp310-linux_x86_64.whl

# Install flashinfer-0.2.2.post1+cu124 (cxx11abi=False)
# vllm-0.8.3 does not support flashinfer>=0.2.3
# see https://github.com/vllm-project/vllm/pull/15777
wget -nv https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.2.post1/flashinfer_python-0.2.2.post1+cu124torch2.6-cp38-abi3-linux_x86_64.whl && \
    pip install --no-cache-dir flashinfer_python-0.2.2.post1+cu124torch2.6-cp38-abi3-linux_x86_64.whl
```
把这两个都下载到本地verl文件夹下，然后把wget那行注释掉（否则会重复下载）。再运行安装脚本。还是会报一个和之前跑demo时安装一样的error，但是安装应该成功了，可以跑了

测试指令：4B模型在80G显卡上很容易OOM。尝试减小ppo_micro_batch_size_per_gpu, ppo_mini_batch_size后，最终采用了如下指令才在单张80G上不OOM：
```bash
PYTHONUNBUFFERED=1 python3 -m verl.trainer.main_ppo \
 data.train_files=$HOME/data/gsm8k/train.parquet \
 data.val_files=$HOME/data/gsm8k/test.parquet \
 data.train_batch_size=256 \
 data.max_prompt_length=512 \
 data.max_response_length=256 \
 actor_rollout_ref.model.path=/root/autodl-tmp/Qwen3-4B-Instruct-2507 \
 actor_rollout_ref.actor.optim.lr=1e-6 \
 actor_rollout_ref.actor.ppo_mini_batch_size=16 \
 actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=1 \
 actor_rollout_ref.rollout.name=vllm \
 actor_rollout_ref.rollout.n=4 \
 actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=8 \
 actor_rollout_ref.rollout.tensor_model_parallel_size=1 \
 actor_rollout_ref.rollout.gpu_memory_utilization=0.4 \
 actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=4 \
 critic.optim.lr=1e-5 \
 critic.model.path=/root/autodl-tmp/Qwen3-4B-Instruct-2507 \
 critic.ppo_micro_batch_size_per_gpu=4 \
 algorithm.kl_ctrl.kl_coef=0.001 \
 algorithm.adv_estimator=grpo \
 trainer.logger=console \
 trainer.val_before_train=False \
 trainer.n_gpus_per_node=1 \
 trainer.nnodes=1 \
 trainer.save_freq=10 \
 trainer.test_freq=10 \
 trainer.total_epochs=15 2>&1 | tee verl_demo.log
```
log如下，7分多种才一个step，很慢了。
```
Training Progress:   0%|                                                                                                             | 0/435 [00:00<?, ?it/s]
(TaskRunner pid=39121) step:1 - global_seqlen/min:285739 - global_seqlen/max:285739 - global_seqlen/minmax_diff:0 - global_seqlen/balanced_min:285739 - global_seqlen/balanced_max:285739 - global_seqlen/mean:285739.0 - actor/entropy:0.08669428527355194 - actor/pg_loss:np.float64(1.5589957911288366e-06) - actor/pg_clipfrac:np.float64(0.0017863540906546405) - actor/ppo_kl:np.float64(0.003328868775039462) - actor/pg_clipfrac_lower:np.float64(4.479644303501118e-06) - actor/grad_norm:np.float64(0.7145342733711004) - perf/mfu/actor:np.float64(0.06478793742797823) - perf/max_memory_allocated_gb:np.float64(88.75690698623657) - perf/max_memory_reserved_gb:np.float64(91.701171875) - perf/cpu_memory_used_gb:np.float64(33.68315505981445) - actor/lr:np.float64(1e-06) - training/global_step:1 - training/epoch:0 - critic/score/mean:0.5859375 - critic/score/max:1.0 - critic/score/min:0.0 - critic/rewards/mean:0.5859375 - critic/rewards/max:1.0 - critic/rewards/min:0.0 - critic/advantages/mean:-0.0059382240287959576 - critic/advantages/max:1.4999970197677612 - critic/advantages/min:-1.4999970197677612 - critic/returns/mean:-0.0059382240287959576 - critic/returns/max:1.4999970197677612 - critic/returns/min:-1.4999970197677612 - response_length/mean:195.3154296875 - response_length/max:256.0 - response_length/min:82.0 - response_length/clip_ratio:0.2724609375 - response_length_non_aborted/mean:195.3154296875 - response_length_non_aborted/max:256.0 - response_length_non_aborted/min:82.0 - response_length_non_aborted/clip_ratio:0.2724609375 - response/aborted_ratio:0.0 - prompt_length/mean:83.7265625 - prompt_length/max:168.0 - prompt_length/min:45.0 - prompt_length/clip_ratio:0.0 - timing_s/start_profile:0.00023921695537865162 - timing_s/generate_sequences:29.87729835510254 - timing_s/generation_timing/max:29.87729835510254 - timing_s/generation_timing/min:29.87729835510254 - timing_s/generation_timing/topk_ratio:0.0 - timing_s/gen:31.85140457795933 - timing_s/reward:0.18974859197624028 - timing_s/old_log_prob:47.182880097068846 - timing_s/adv:0.025264299008995295 - timing_s/update_actor:381.57870407693554 - timing_s/step:460.8668458350003 - timing_s/stop_profile:6.380595732480288e-05 - timing_per_token_ms/update_actor:1.3354099513084863 - timing_per_token_ms/gen:0.1592546340702856 - timing_per_token_ms/adv:8.841739842651963e-05 - perf/total_num_tokens:285739 - perf/time_per_step:460.8668458350003 - perf/throughput:620.0033753400008
Training Progress:   0%|▏                                                                                                | 1/435 [07:40<55:30:33, 460.45s/it]
(TaskRunner pid=39121) step:2 - global_seqlen/min:286566 - global_seqlen/max:286566 - global_seqlen/minmax_diff:0 - global_seqlen/balanced_min:286566 - global_seqlen/balanced_max:286566 - global_seqlen/mean:286566.0 - actor/entropy:0.08217394351959229 - actor/pg_loss:np.float64(1.4693619050376583e-06) - actor/pg_clipfrac:np.float64(0.001324377769833518) - actor/ppo_kl:np.float64(0.0012399085543188804) - actor/pg_clipfrac_lower:np.float64(5.336406957212603e-06) - actor/grad_norm:np.float64(1.211572265252471) - perf/mfu/actor:np.float64(0.06494146270048892) - perf/max_memory_allocated_gb:np.float64(88.75690698623657) - perf/max_memory_reserved_gb:np.float64(91.740234375) - perf/cpu_memory_used_gb:np.float64(33.84892272949219) - actor/lr:np.float64(1e-06) - training/global_step:2 - training/epoch:0 - critic/score/mean:0.70703125 - critic/score/max:1.0 - critic/score/min:0.0 - critic/rewards/mean:0.70703125 - critic/rewards/max:1.0 - critic/rewards/min:0.0 - critic/advantages/mean:-0.00966594647616148 - critic/advantages/max:1.4999970197677612 - critic/advantages/min:-1.4999970197677612 - critic/returns/mean:-0.00966594647616148 - critic/returns/max:1.4999970197677612 - critic/returns/min:-1.4999970197677612 - response_length/mean:195.751953125 - response_length/max:256.0 - response_length/min:82.0 - response_length/clip_ratio:0.259765625 - response_length_non_aborted/mean:195.751953125 - response_length_non_aborted/max:256.0 - response_length_non_aborted/min:82.0 - response_length_non_aborted/clip_ratio:0.259765625 - response/aborted_ratio:0.0 - prompt_length/mean:84.09765625 - prompt_length/max:194.0 - prompt_length/min:44.0 - prompt_length/clip_ratio:0.0 - timing_s/start_profile:5.728495307266712e-05 - timing_s/generate_sequences:30.368087768554688 - timing_s/generation_timing/max:30.368087768554688 - timing_s/generation_timing/min:30.368087768554688 - timing_s/generation_timing/topk_ratio:0.0 - timing_s/gen:33.03782230708748 - timing_s/reward:0.18279346299823374 - timing_s/old_log_prob:47.785105952993035 - timing_s/adv:0.026053201989270747 - timing_s/update_actor:381.8128314119531 - timing_s/step:462.8791977239307 - timing_s/stop_profile:6.0086022131145e-05 - timing_per_token_ms/update_actor:1.3323731057137032 - timing_per_token_ms/gen:0.16481827042697672 - timing_per_token_ms/adv:9.091518878468048e-05 - perf/total_num_tokens:286566 - perf/time_per_step:462.8791977239307 - perf/throughput:619.0945745868515
Training Progress:   0%|▍                                                                                                | 2/435 [15:23<55:33:14, 461.88s/it]
(TaskRunner pid=39121) step:3 - global_seqlen/min:281326 - global_seqlen/max:281326 - global_seqlen/minmax_diff:0 - global_seqlen/balanced_min:281326 - global_seqlen/balanced_max:281326 - global_seqlen/mean:281326.0 - actor/entropy:0.08351734280586243 - actor/pg_loss:np.float64(1.0229255167359952e-06) - actor/pg_clipfrac:np.float64(0.0010350689503866306) - actor/ppo_kl:np.float64(0.000943890276035475) - actor/pg_clipfrac_lower:np.float64(0.0) - actor/grad_norm:np.float64(0.5680828988552094) - perf/mfu/actor:np.float64(0.06371384682172776) - perf/max_memory_allocated_gb:np.float64(88.75690698623657) - perf/max_memory_reserved_gb:np.float64(91.740234375) - perf/cpu_memory_used_gb:np.float64(34.27031326293945) - actor/lr:np.float64(1e-06) - training/global_step:3 - training/epoch:0 - critic/score/mean:0.73046875 - critic/score/max:1.0 - critic/score/min:0.0 - critic/rewards/mean:0.73046875 - critic/rewards/max:1.0 - critic/rewards/min:0.0 - critic/advantages/mean:-0.007370196748524904 - critic/advantages/max:1.4999970197677612 - critic/advantages/min:-1.4999970197677612 - critic/returns/mean:-0.007370196748524904 - critic/returns/max:1.4999970197677612 - critic/returns/min:-1.4999970197677612 - response_length/mean:191.509765625 - response_length/max:256.0 - response_length/min:81.0 - response_length/clip_ratio:0.2255859375 - response_length_non_aborted/mean:191.509765625 - response_length_non_aborted/max:256.0 - response_length_non_aborted/min:81.0 - response_length_non_aborted/clip_ratio:0.2255859375 - response/aborted_ratio:0.0 - prompt_length/mean:83.22265625 - prompt_length/max:169.0 - prompt_length/min:46.0 - prompt_length/clip_ratio:0.0 - timing_s/start_profile:5.97670441493392e-05 - timing_s/generate_sequences:29.526098251342773 - timing_s/generation_timing/max:29.526098251342773 - timing_s/generation_timing/min:29.526098251342773 - timing_s/generation_timing/topk_ratio:0.0 - timing_s/gen:32.064561213948764 - timing_s/reward:0.18320233991835266 - timing_s/old_log_prob:47.72964694094844 - timing_s/adv:0.024790439987555146 - timing_s/update_actor:381.88847792102024 - timing_s/step:461.92554280604236 - timing_s/stop_profile:6.18878984823823e-05 - timing_per_token_ms/update_actor:1.3574588837186048 - timing_per_token_ms/gen:0.1635062732091255 - timing_per_token_ms/adv:8.8119974647047e-05 - perf/total_num_tokens:281326 - perf/time_per_step:461.92554280604236 - perf/throughput:609.0288887058272
Training Progress:   1%|▋                                                                                                | 3/435 [23:05<55:25:43, 461.91s/it]
(TaskRunner pid=39121) step:4 - global_seqlen/min:282191 - global_seqlen/max:282191 - global_seqlen/minmax_diff:0 - global_seqlen/balanced_min:282191 - global_seqlen/balanced_max:282191 - global_seqlen/mean:282191.0 - actor/entropy:0.08523262292146683 - actor/pg_loss:np.float64(1.1438846740929876e-06) - actor/pg_clipfrac:np.float64(0.0010702955560191185) - actor/ppo_kl:np.float64(0.0007858721004870839) - actor/pg_clipfrac_lower:np.float64(0.0) - actor/grad_norm:np.float64(0.5055269617587328) - perf/mfu/actor:np.float64(0.06353472777022628) - perf/max_memory_allocated_gb:np.float64(88.75690698623657) - perf/max_memory_reserved_gb:np.float64(91.740234375) - perf/cpu_memory_used_gb:np.float64(34.79139709472656) - actor/lr:np.float64(1e-06) - training/global_step:4 - training/epoch:0 - critic/score/mean:0.693359375 - critic/score/max:1.0 - critic/score/min:0.0 - critic/rewards/mean:0.693359375 - critic/rewards/max:1.0 - critic/rewards/min:0.0 - critic/advantages/mean:-0.008578773587942123 - critic/advantages/max:1.4999970197677612 - critic/advantages/min:-1.4999970197677612 - critic/returns/mean:-0.008578773587942123 - critic/returns/max:1.4999970197677612 - critic/returns/min:-1.4999970197677612 - response_length/mean:192.9130859375 - response_length/max:256.0 - response_length/min:79.0 - response_length/clip_ratio:0.283203125 - response_length_non_aborted/mean:192.9130859375 - response_length_non_aborted/max:256.0 - response_length_non_aborted/min:79.0 - response_length_non_aborted/clip_ratio:0.283203125 - response/aborted_ratio:0.0 - prompt_length/mean:82.6640625 - prompt_length/max:185.0 - prompt_length/min:50.0 - prompt_length/clip_ratio:0.0 - timing_s/start_profile:6.137602031230927e-05 - timing_s/generate_sequences:29.665102005004883 - timing_s/generation_timing/max:29.665102005004883 - timing_s/generation_timing/min:29.665102005004883 - timing_s/generation_timing/topk_ratio:0.0 - timing_s/gen:32.1642507519573 - timing_s/reward:0.18205717694945633 - timing_s/old_log_prob:47.746868365909904 - timing_s/adv:0.024188114097341895 - timing_s/update_actor:384.2175700060325 - timing_s/step:464.3691727500409 - timing_s/stop_profile:5.8886012993752956e-05 - timing_per_token_ms/update_actor:1.3615514669356303 - timing_per_token_ms/gen:0.16282151608488935 - timing_per_token_ms/adv:8.571539878076159e-05 - perf/total_num_tokens:282191 - perf/time_per_step:464.3691727500409 - perf/throughput:607.686764237248
Training Progress:   1%|▉                                                                                                | 4/435 [30:49<55:25:01, 462.88s/it]
```

merge指令：
```
python3 -m verl.model_merger merge \
--backend fsdp \
--local_dir checkpoints/verl_examples/gsm8k/global_step_30/actor \
--target_dir /root/autodl-tmp/final_model
```
合并成功

为了后面的实验方便进行（需要添加Reward Model，以及7B的模型训练，还需要更长的response_length），还是得租更多的卡。这里直接又租了3卡，共4卡A800

尝试指令：
```
PYTHONUNBUFFERED=1 python3 -m verl.trainer.main_ppo \
 data.train_files=$HOME/data/style_imitation/sft_dataset_final.parquet \
 data.val_files=$HOME/data/style_imitation/val_dataset_final.parquet \
 data.train_batch_size=256 \
 data.max_prompt_length=256 \
 data.max_response_length=512 \
 actor_rollout_ref.model.path=/root/autodl-tmp/Qwen3-4B-Instruct-2507-final \
 actor_rollout_ref.actor.optim.lr=1e-6 \
 actor_rollout_ref.actor.ppo_mini_batch_size=16 \
 actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=1 \
 actor_rollout_ref.rollout.name=vllm \
 actor_rollout_ref.rollout.n=4 \
 actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=8 \
 actor_rollout_ref.rollout.tensor_model_parallel_size=1 \
 actor_rollout_ref.rollout.gpu_memory_utilization=0.4 \
 actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=4 \
 critic.optim.lr=1e-5 \
 critic.model.path=/root/autodl-tmp/Qwen3-4B-Instruct-2507 \
 critic.ppo_micro_batch_size_per_gpu=4 \
 algorithm.kl_ctrl.kl_coef=0.001 \
 algorithm.adv_estimator=grpo \
 trainer.logger=[console,wandb] \
 trainer.project_name='verl_grpo_elemental_equation' \
 trainer.experiment_name='qwen3_4b_grpo' \
 trainer.val_before_train=False \
 trainer.n_gpus_per_node=4 \
 trainer.nnodes=1 \
 trainer.save_freq=1 \
 trainer.test_freq=1 \
 reward_model.enable=True \
 reward_model.model.path=/root/autodl-tmp/Skywork-Reward-V2-Qwen3-4B-finetuned \
 reward_model.micro_batch_size_per_gpu=8 \
 trainer.total_epochs=1 2>&1 | tee verl_grpo_qwen3_4B.log
```

初始化等了许久之后崩了。似乎是多卡之间通讯有问题。

```
Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
(/root/autodl-tmp/verl-env) root@autodl-container-c6d54aa471-1d603b3b:~/autodl-tmp/verl# nvidia-smi topo -m
        GPU0    GPU1    GPU2    GPU3    NIC0    NIC1    NIC2    CPU Affinity    NUMA Affinity   GPU NUMA ID
GPU0     X      PXB     PXB     SYS     NODE    PXB     SYS     0-27,56-83      0               N/A
GPU1    PXB      X      PXB     SYS     NODE    PIX     SYS     0-27,56-83      0               N/A
GPU2    PXB     PXB      X      SYS     NODE    PXB     SYS     0-27,56-83      0               N/A
GPU3    SYS     SYS     SYS      X      SYS     SYS     PXB     28-55,84-111    1               N/A
NIC0    NODE    NODE    NODE    SYS      X      NODE    SYS
NIC1    PXB     PIX     PXB     SYS     NODE     X      SYS
NIC2    SYS     SYS     SYS     PXB     SYS     SYS      X 

Legend:

  X    = Self
  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)
  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node
  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)
  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)
  PIX  = Connection traversing at most a single PCIe bridge
  NV#  = Connection traversing a bonded set of # NVLinks

NIC Legend:

  NIC0: mlx5_0
  NIC1: mlx5_1
  NIC2: mlx5_bond_0
```
GPU 4和另三块相连的效果很差。gemini让我只用三块：

```
PYTHONUNBUFFERED=1 python3 -m verl.trainer.main_ppo \
 data.train_files=$HOME/data/style_imitation/sft_dataset_final.parquet \
 data.val_files=$HOME/data/style_imitation/val_dataset_final.parquet \
 data.train_batch_size=255 \
 data.max_prompt_length=256 \
 data.max_response_length=512 \
 actor_rollout_ref.model.path=/root/autodl-tmp/Qwen3-4B-Instruct-2507-final \
 actor_rollout_ref.actor.optim.lr=1e-6 \
 actor_rollout_ref.actor.ppo_mini_batch_size=16 \
 actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=1 \
 actor_rollout_ref.rollout.name=vllm \
 actor_rollout_ref.rollout.n=4 \
 actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=8 \
 actor_rollout_ref.rollout.tensor_model_parallel_size=1 \
 actor_rollout_ref.rollout.gpu_memory_utilization=0.2 \
 actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=4 \
 critic.optim.lr=1e-5 \
 critic.model.path=/root/autodl-tmp/Qwen3-4B-Instruct-2507 \
 critic.ppo_micro_batch_size_per_gpu=4 \
 algorithm.kl_ctrl.kl_coef=0.001 \
 algorithm.adv_estimator=grpo \
 trainer.logger=[console,wandb] \
 trainer.project_name='verl_grpo_elemental_equation' \
 trainer.experiment_name='qwen3_4b_grpo' \
 trainer.val_before_train=False \
 trainer.n_gpus_per_node=3 \
 trainer.nnodes=1 \
 trainer.save_freq=1 \
 trainer.test_freq=1 \
 reward_model.enable=True \
 reward_model.model.path=/root/autodl-tmp/Skywork-Reward-V2-Qwen3-4B-finetuned \
 reward_model.micro_batch_size_per_gpu=8 \
 trainer.total_epochs=1 2>&1 | tee verl_grpo_qwen3_4B.log
```

还是超时了。打开NCCL日志模式再跑一遍看看是什么情况：
```
export NCCL_DEBUG=INFO
```
试过若干Gemini给的建议后都无果。只要多GPU必阻塞。阻塞时所有GPU占用100%

不死心，因为之前在2卡3090上成功跑过单机多卡。网上搜到了https://zhuanlan.zhihu.com/p/607203976 说A100/A800会导致卡死。又赌徒般地租了2卡H20，用同样装环境的方式，在同样的模型上跑同样的脚本，结果却成功了！！！最终问题可能确实出在A100/800上

尝试指令：
```
PYTHONUNBUFFERED=1 python3 -m verl.trainer.main_ppo \
 data.train_files=$HOME/data/style_imitation/sft_dataset_final.parquet \
 data.val_files=$HOME/data/style_imitation/val_dataset_final.parquet \
 data.train_batch_size=256 \
 data.max_prompt_length=256 \
 data.max_response_length=512 \
 data.return_raw_chat=True \
 actor_rollout_ref.model.path=/root/autodl-tmp/Qwen3-4B-Instruct-2507-final \
 actor_rollout_ref.actor.optim.lr=1e-6 \
 actor_rollout_ref.actor.ppo_mini_batch_size=16 \
 actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=1 \
 actor_rollout_ref.rollout.name=vllm \
 actor_rollout_ref.rollout.n=4 \
 actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=8 \
 actor_rollout_ref.rollout.tensor_model_parallel_size=1 \
 actor_rollout_ref.rollout.gpu_memory_utilization=0.4 \
 actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=4 \
 critic.optim.lr=1e-5 \
 critic.model.path=/root/autodl-tmp/Qwen3-4B-Instruct-2507 \
 critic.ppo_micro_batch_size_per_gpu=4 \
 algorithm.kl_ctrl.kl_coef=0.001 \
 algorithm.adv_estimator=grpo \
 trainer.logger=[console,wandb] \
 trainer.project_name='verl_grpo_elemental_equation' \
 trainer.experiment_name='qwen3_4b_grpo' \
 trainer.val_before_train=False \
 trainer.n_gpus_per_node=2 \
 trainer.nnodes=1 \
 trainer.save_freq=1 \
 trainer.test_freq=1 \
 reward_model.enable=True \
 reward_model.model.path=/root/autodl-tmp/Skywork-Reward-V2-Qwen3-4B-finetuned \
 reward_model.micro_batch_size_per_gpu=8 \
 trainer.total_epochs=1 2>&1 | tee verl_grpo_qwen3_4B.log
```
成功了。
进行merge：
```
python3 -m verl.model_merger merge \
--backend fsdp \
--local_dir checkpoints/verl_grpo_elemental_equation/qwen3_4b_grpo/global_step_2/actor \
--target_dir /root/autodl-tmp/qwen3_4b_instruct_2507_grpo
```

Qwen2.5-7B的训练：
```
PYTHONUNBUFFERED=1 python3 -m verl.trainer.main_ppo \
 data.train_files=$HOME/data/style_imitation/sft_dataset_final.parquet \
 data.val_files=$HOME/data/style_imitation/val_dataset_final.parquet \
 data.train_batch_size=256 \
 data.max_prompt_length=256 \
 data.max_response_length=512 \
 data.return_raw_chat=True \
 actor_rollout_ref.model.path=/root/autodl-tmp/Qwen2.5-7B-Instruct-sft-final-new \
 actor_rollout_ref.actor.optim.lr=1e-6 \
 actor_rollout_ref.actor.ppo_mini_batch_size=16 \
 actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=1 \
 actor_rollout_ref.rollout.name=vllm \
 actor_rollout_ref.rollout.n=4 \
 actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=8 \
 actor_rollout_ref.rollout.tensor_model_parallel_size=1 \
 actor_rollout_ref.rollout.gpu_memory_utilization=0.4 \
 actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=4 \
 critic.optim.lr=1e-5 \
 critic.model.path=/root/autodl-tmp/Qwen2.5-7B-Instruct-sft-final-new \
 critic.ppo_micro_batch_size_per_gpu=4 \
 algorithm.kl_ctrl.kl_coef=0.001 \
 algorithm.adv_estimator=grpo \
 trainer.logger=[console,wandb] \
 trainer.project_name='verl_grpo_elemental_equation' \
 trainer.experiment_name='qwen2_5_7b_grpo' \
 trainer.val_before_train=False \
 trainer.n_gpus_per_node=2 \
 trainer.nnodes=1 \
 trainer.save_freq=1 \
 trainer.test_freq=1 \
 reward_model.enable=True \
 reward_model.model.path=/root/autodl-tmp/Skywork-Reward-V2-Qwen3-4B-finetuned \
 reward_model.micro_batch_size_per_gpu=8 \
 trainer.total_epochs=1 2>&1 | tee verl_grpo_qwen2_5_7B.log
```
合并：
```
python3 -m verl.model_merger merge \
--backend fsdp \
--local_dir checkpoints/verl_grpo_elemental_equation/qwen2_5_7b_grpo/global_step_2/actor \
--target_dir /root/autodl-tmp/qwen2_5_7b_instruct_grpo
```
### 最终评估记录
9.5记录：
GRPO后的Qwen2.5，使用new judge评测：

```
按内容类型统计:
                          avg_score
                              count  mean
content_type
ecommerce_long_form              19  4.51
paid_ad_bab                      19  4.16
paid_ad_cta                      19  4.73
paid_ad_fab                      19  4.03
paid_ad_pas                      19  4.36
social_media_educational         19  4.33
social_media_myth_busting        18  4.64
social_media_review              19  4.45
social_media_storytelling        19  4.47

整体平均分: 4.41

各维度平均分:
  score_authenticity_immersion: 3.73
  score_value_engagement: 4.31
  score_brand_fit: 4.84
  score_instruction_following: 4.97
  score_professionalism_persuasion: 4.26
  score_clarity_value: 5.00
  score_trust_building: 4.26
  score_suitability_brevity: 3.89
  score_click_appeal: 4.00
  score_cta_effectiveness: 3.75
  score_factual_accuracy: 5.00
```

使用老的judge直接评测三个的记录：（对Qwen2.5-7B-Instruct的工作）
```
=== 评测统计报告 ===
总评测数据: 170 条

按内容类型统计:
                           best_model  base_avg_score  sft_avg_score  grpo_avg_score
content_type
ecommerce_long_form                19            3.47           4.61            4.21
paid_ad_bab                        19            4.06           4.54            4.53
paid_ad_cta                        19            4.29           4.53            4.41
paid_ad_fab                        19            3.68           4.43            4.63
paid_ad_pas                        19            3.65           4.73            4.53
social_media_educational           19            3.26           4.62            4.50
social_media_myth_busting          18            3.38           4.67            4.53
social_media_review                19            3.39           4.07            4.72
social_media_storytelling          19            3.33           4.38            4.70

最佳模型统计:
  B: 88 次 (51.8%)
  C: 80 次 (47.1%)
  A: 2 次 (1.2%)

整体平均分:
  Base (基础模型): 3.62
  SFT (微调模型): 4.51
  GRPO (强化学习模型): 4.53
```

在Qwen3上做了同样的实验，效果似乎并不理想：
```
=== 评测统计报告 ===
总评测数据: 170 条

按内容类型统计:
                           best_model  base_avg_score  sft_avg_score  grpo_avg_score
content_type
ecommerce_long_form                19            4.51           4.37            3.60
paid_ad_bab                        19            3.85           4.75            4.60
paid_ad_cta                        19            4.44           4.63            4.43
paid_ad_fab                        19            4.20           4.71            4.45
paid_ad_pas                        19            4.34           4.68            4.53
social_media_educational           19            4.21           4.17            4.61
social_media_myth_busting          18            4.65           3.92            4.42
social_media_review                19            4.63           4.26            3.84
social_media_storytelling          19            4.37           4.12            4.32

最佳模型统计:
  B: 72 次 (42.4%)
  C: 60 次 (35.3%)
  A: 38 次 (22.4%)

整体平均分:
  Base (基础模型): 4.35
  SFT (微调模型): 4.40
  GRPO (强化学习模型): 4.31
```

Qwen3的基模太强大了，微调没什么新增加了

GRPO后Qwen3-4B的结果：
```
=== 评测统计报告 ===
总评测数据: 170 条

按内容类型统计:
                          avg_score
                              count  mean
content_type
ecommerce_long_form              19  4.88
paid_ad_bab                      19  4.49
paid_ad_cta                      19  5.00
paid_ad_fab                      19  4.42
paid_ad_pas                      19  4.84
social_media_educational         19  4.68
social_media_myth_busting        18  4.68
social_media_review              19  4.63
social_media_storytelling        19  4.66

整体平均分: 4.70

各维度平均分:
  score_authenticity_immersion: 4.04
  score_value_engagement: 4.68
  score_brand_fit: 4.93
  score_instruction_following: 4.98
  score_professionalism_persuasion: 4.89
  score_clarity_value: 5.00
  score_trust_building: 4.74
  score_suitability_brevity: 4.61
  score_click_appeal: 4.59
  score_cta_effectiveness: 4.29
  score_factual_accuracy: 5.00
```
比使用这个评测脚本的SFT模型的4.67还是涨了一点点。
有一点随机性，不过差别也不大。

