"""
ä½¿ç”¨ TRL åº“å¯¹ Qwen2.5-0.5B è¿›è¡Œ LoRA SFT çš„è®­ç»ƒè„šæœ¬
"""

import os
import torch
from datasets import Dataset, load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    DataCollatorForSeq2Seq,
)
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer, SFTConfig
import json


def load_and_prepare_dataset(data_path, tokenizer, max_length=512):
    """
    åŠ è½½å’Œé¢„å¤„ç†æ•°æ®é›†
    æ”¯æŒ JSON Lines æ ¼å¼çš„æ•°æ®ï¼Œæ•°æ®æ ¼å¼ä¸º {"prompt": "...", "chosen": "..."}
    """
    
    # ç¤ºä¾‹æ•°æ®æ ¼å¼ - å¦‚æœæ²¡æœ‰æä¾›æ•°æ®è·¯å¾„ï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®
    if not os.path.exists(data_path):
        print(f"æ•°æ®æ–‡ä»¶ {data_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®")
        sample_data = [
            {
                "prompt": "è¯·ä»‹ç»ä¸€ä¸‹åŒ—äº¬çš„å†å²",
                "chosen": "åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½ï¼Œæœ‰ç€æ‚ ä¹…çš„å†å²ã€‚ä½œä¸ºå…­æœå¤éƒ½ï¼ŒåŒ—äº¬æ‹¥æœ‰ä¸°å¯Œçš„æ–‡åŒ–é—äº§å’Œå†å²å»ºç­‘ã€‚"
            },
            {
                "prompt": "è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
                "chosen": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒè®©è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ”¹è¿›ã€‚"
            }
        ]
        dataset = Dataset.from_list(sample_data)
    else:
        # åŠ è½½ JSON Lines æ ¼å¼çš„æ•°æ®
        with open(data_path, 'r', encoding='utf-8') as f:
            data = [json.loads(line) for line in f]
        dataset = Dataset.from_list(data)
    
    def format_prompt(example):
        """æ ¼å¼åŒ–è¾“å…¥æç¤ºï¼Œå°†chosenå­—æ®µé‡å‘½åä¸ºcompletionä»¥ç¬¦åˆSFTTraineræœŸæœ›"""
        # é‡å‘½åå­—æ®µä»¥ç¬¦åˆSFTTrainerçš„æœŸæœ›
        return {
            "prompt": example["prompt"],
            "completion": example["chosen"]  # å°†chosené‡å‘½åä¸ºcompletion
        }
    
    # æ ¼å¼åŒ–æ•°æ®é›†
    formatted_dataset = dataset.map(format_prompt)
    
    return formatted_dataset


def create_model_and_tokenizer(model_name):
    """
    åˆ›å»ºæ¨¡å‹å’Œåˆ†è¯å™¨
    """
    
    # åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True,
        padding_side="right"
    )
    
    # è®¾ç½® pad_token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨ fp16 ç²¾åº¦ï¼‰
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",
        trust_remote_code=True,
        torch_dtype=torch.float16,
    )
    
    # ç¡®ä¿æ¨¡å‹å’Œtokenizerå…³è”
    model.config.pad_token_id = tokenizer.pad_token_id
    
    return model, tokenizer


def setup_lora_config():
    """
    è®¾ç½® LoRA é…ç½®
    """
    lora_config = LoraConfig(
        r=16,                    # LoRA ç§©
        lora_alpha=32,           # LoRA ç¼©æ”¾å‚æ•°
        target_modules=[         # ç›®æ ‡æ¨¡å—
            "q_proj",
            "k_proj", 
            "v_proj",
            "o_proj",
            "gate_proj",
            "up_proj",
            "down_proj",
        ],
        lora_dropout=0.1,        # LoRA dropout
        bias="none",             # åç½®è®¾ç½®
        task_type="CAUSAL_LM",   # ä»»åŠ¡ç±»å‹
    )
    
    return lora_config


def main():
    """
    ä¸»è®­ç»ƒå‡½æ•°
    """
    
    # é…ç½®å‚æ•°
    MODEL_NAME = "models/Qwen2.5-0.5B-Instruct"  # æ¨¡å‹åç§°
    DATA_PATH = "data/rm_dataset_final.jsonl"     # è®­ç»ƒæ•°æ®è·¯å¾„
    OUTPUT_DIR = "models/qwen2.5-0.5b-lora-sft"  # è¾“å‡ºç›®å½•
    MAX_LENGTH = 512                   # æœ€å¤§åºåˆ—é•¿åº¦
    
    print("å¼€å§‹åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
    
    # åˆ›å»ºæ¨¡å‹å’Œåˆ†è¯å™¨
    model, tokenizer = create_model_and_tokenizer(MODEL_NAME)
    
    print("è®¾ç½® LoRA é…ç½®...")
    
    # è®¾ç½® LoRA é…ç½®
    lora_config = setup_lora_config()
    
    # å°† LoRA åº”ç”¨åˆ°æ¨¡å‹
    model = get_peft_model(model, lora_config)
    
    # æ‰“å°å¯è®­ç»ƒå‚æ•°
    model.print_trainable_parameters()
    
    print("åŠ è½½å’Œå¤„ç†æ•°æ®é›†...")
    
    # åŠ è½½æ•°æ®é›†
    train_dataset = load_and_prepare_dataset(DATA_PATH, tokenizer, MAX_LENGTH)
    
    print(f"è®­ç»ƒæ•°æ®é›†å¤§å°: {len(train_dataset)}")
    
    # è®­ç»ƒå‚æ•° - ä½¿ç”¨ SFTConfig æ›¿ä»£ TrainingArguments
    sft_config = SFTConfig(
        output_dir=OUTPUT_DIR,
        per_device_train_batch_size=4,
        per_device_eval_batch_size=4,
        gradient_accumulation_steps=4,
        num_train_epochs=3,
        learning_rate=2e-4,
        fp16=True,
        logging_steps=10,
        save_steps=100,
        save_total_limit=2,
        remove_unused_columns=False,
        dataloader_drop_last=True,
        warmup_steps=100,
        weight_decay=0.01,
        lr_scheduler_type="cosine",
        report_to=None,  # ä¸ä½¿ç”¨ wandb ç­‰è®°å½•å·¥å…·
        # SFT ç‰¹å®šå‚æ•°
        max_length=MAX_LENGTH,
        packing=False,  # ä¸è¿›è¡Œåºåˆ—æ‰“åŒ…
        dataset_num_proc=2,
    )
    
    print("åˆå§‹åŒ– SFT è®­ç»ƒå™¨...")
    
    # åˆ›å»º SFT è®­ç»ƒå™¨
    trainer = SFTTrainer(
        model=model,
        data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),
        train_dataset=train_dataset,
        args=sft_config,
    )
    
    print("å¼€å§‹è®­ç»ƒ...")
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train()
    
    print("ä¿å­˜æ¨¡å‹...")
    
    # ä¿å­˜ LoRA é€‚é…å™¨
    model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    
    print(f"è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ°: {OUTPUT_DIR}")


def create_sample_data():
    """
    åˆ›å»ºç¤ºä¾‹è®­ç»ƒæ•°æ®æ–‡ä»¶
    """
    sample_data = [
        {
            "prompt": "è¯·ä¸º\"åŸç´ æ–¹ç¨‹\"å“ç‰Œçš„å¦‚ä¸‹äº§å“åˆ›ä½œä¸€æ¡ä¾§é‡ä½¿ç”¨åé¦ˆçš„å°çº¢ä¹¦æ–‡æ¡ˆ\n\n{\"product_name\": \"å‡€é¢œç„•é‡‡ç²¾å\", \"category\": \"ç²¾å\", \"core_ingredients\": \"10%çƒŸé…°èƒº, é”ŒPCA, çº¢æ²¡è¯é†‡\", \"features\": \"é«˜æµ“åº¦çƒŸé…°èƒº, æºå¤´æ§æ²¹, æ”¹å–„æš—æ²‰\", \"target_audience\": \"æ²¹æ€§ç—˜ç—˜è‚Œ\"}",
            "chosen": "æ²¹ç—˜è‚Œçš„å¤æ—¥æ•‘æ˜Ÿâœ¨ è¿™æ”¯ç²¾åè®©æˆ‘å‘Šåˆ«'åå…‰è„¸'ï¼\n\næ¯åˆ°å¤å¤©ï¼Œæˆ‘çš„TåŒºå°±åƒä¸ªå°å‹æ²¹ç”°ğŸŒ‹ å¦†å2å°æ—¶å°±å¼€å§‹è„±å¦†ï¼Œæ¯›å­”é‡Œè¿˜èƒ½æŒ¤å‡ºç™½è‰²è§’æ “...ç›´åˆ°é‡åˆ°åŸç´ æ–¹ç¨‹çš„å‡€é¢œç„•é‡‡ç²¾åï¼Œç»ˆäºæ˜ç™½ä»€ä¹ˆå«æºå¤´æ§æ²¹çš„ç§‘å­¦é…æ–¹ã€‚\n\nğŸ§ªæ ¸å¿ƒæˆåˆ†æ‹†è§£ï¼š\nâ€¢ 10%çƒŸé…°èƒºï¼šä¸æ˜¯ç›²ç›®å †æµ“åº¦ï¼è¿™ä¸ªé»„é‡‘é…æ¯”æ—¢èƒ½æŠ‘åˆ¶çš®è„‚è…ºæ´»è·ƒåº¦ï¼Œåˆä¸ä¼šåˆºæ¿€å±éšœ\nâ€¢ é”ŒPCAï¼šåƒå¸æ²¹çº¸ä¸€æ ·å¸é™„è¡¨é¢æ²¹è„‚ï¼Œä½†ä¸ä¼šæ‹”å¹²\nâ€¢ çº¢æ²¡è¯é†‡ï¼šç»™èºåŠ¨çš„ç—˜ç—˜è‚Œ'ç­ç«'ï¼Œæˆ‘ç»æœŸçˆ†ç—˜æ—¶åšæ¶‚å®ƒï¼Œçº¢è‚¿æ¶ˆé€€é€Ÿåº¦è‚‰çœ¼å¯è§"
        },
        {
            "prompt": "è¯·è§£é‡Šä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ",
            "chosen": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œå®ƒåŸºäºäººå·¥ç¥ç»ç½‘ç»œï¼Œç‰¹åˆ«æ˜¯æ·±å±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„è¡¨ç¤ºã€‚æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è¯­éŸ³è¯†åˆ«ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆæœã€‚"
        },
        {
            "prompt": "å¦‚ä½•å­¦ä¹ Pythonç¼–ç¨‹ï¼Ÿ",
            "chosen": "å­¦ä¹ Pythonç¼–ç¨‹å»ºè®®éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š1. æŒæ¡åŸºç¡€è¯­æ³•å’Œæ•°æ®ç±»å‹ï¼›2. ç»ƒä¹ ç¼–å†™ç®€å•ç¨‹åºï¼›3. å­¦ä¹ é¢å‘å¯¹è±¡ç¼–ç¨‹ï¼›4. æ¢ç´¢å¸¸ç”¨åº“å¦‚NumPyã€Pandasç­‰ï¼›5. é€šè¿‡å®é™…é¡¹ç›®å·©å›ºçŸ¥è¯†ã€‚"
        },
        {
            "prompt": "ç¿»è¯‘ä»¥ä¸‹è‹±æ–‡ï¼šHello, how are you today?",
            "chosen": "ä½ å¥½ï¼Œä½ ä»Šå¤©æ€ä¹ˆæ ·ï¼Ÿ"
        }
    ]
    
    with open("data/rm_dataset_final.jsonl", "w", encoding="utf-8") as f:
        for item in sample_data:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
    
    print("ç¤ºä¾‹æ•°æ®æ–‡ä»¶ rm_dataset_final.jsonl å·²åˆ›å»º")


if __name__ == "__main__":
    # åˆ›å»ºç¤ºä¾‹æ•°æ®ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if not os.path.exists("data/rm_dataset_final.jsonl"):
        print("åˆ›å»ºç¤ºä¾‹è®­ç»ƒæ•°æ®...")
        create_sample_data()
    
    # è¿è¡Œè®­ç»ƒ
    main()